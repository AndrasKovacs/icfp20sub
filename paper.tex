
%% build: latexmk -pdf -pvc -xelatex paper.tex

%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,prologue,dvipsnames]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


%% ------------------------------------------------------------

\usepackage{xcolor}
\usepackage{mathpartir}
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}
\usepackage{scalerel}
\usepackage{amssymb}

%% \usepackage{cleveref}
%% \usepackage{hyperref}
%% \usepackage{nameref}

\newcommand{\slet}{\boldsymbol{\mathsf{let}}}
\renewcommand{\sin}{\boldsymbol{\mathsf{in}}}
\renewcommand{\U}{\mathsf{U}}
\newcommand{\emptycon}{\scaleobj{.75}\bullet}
\newcommand{\To}{\Rightarrow}
\newcommand{\p}{\mathsf{p}}
\newcommand{\id}{\mathsf{id}}
\newcommand{\blank}{\mathord{\hspace{1pt}\text{--}\hspace{1pt}}}
\newcommand{\freshMeta}[3]{\mathsf{freshMeta}\,_{#1|#2}\,#3}
\newcommand{\constancy}{\mathsf{constancy}}
\newcommand{\newConstancy}[5]{\mathsf{newConstancy}_{#1|#2,\,#3 : \Rec\,#4}\,#5}
\newcommand{\unify}{\mathsf{unify}}
\newcommand{\fail}{\mathsf{fail}}
\newcommand{\kw}[1]{{\mathsf{#1}}}
\newcommand{\FreeVars}{\mathsf{FreeVars}}

\newcommand{\echeckblank}{\llbracket\blank\rrbracket\!\Downarrow}
\newcommand{\einferblank}{\llbracket\blank\rrbracket\!\Uparrow}
\newcommand{\echeck}[4]{\llbracket#1\rrbracket\!\Downarrow\,_{#2|#3}\,#4}
\newcommand{\einfer}[3]{\llbracket#1\rrbracket\!\Uparrow\,_{#2|#3}}
\newcommand{\echeckt}[2]{\llbracket#1\rrbracket\!\Downarrow\,#2}
\newcommand{\einfert}[1]{\llbracket#1\rrbracket\!\Uparrow}
\newcommand{\edo}{\boldsymbol{\mathsf{do}}}
\newcommand{\ereturn}{\boldsymbol{\mathsf{return}}}
\newcommand{\eif}{\boldsymbol{\mathsf{if}}}
\newcommand{\ethen}{\boldsymbol{\mathsf{then}}}
\newcommand{\eelse}{\boldsymbol{\mathsf{else}}}
\newcommand{\ecase}{\boldsymbol{\mathsf{case}}}
\newcommand{\eof}{\boldsymbol{\mathsf{of}}}
\newcommand{\true}{\mathsf{true}}
\newcommand{\false}{\mathsf{false}}
\newcommand{\einsert}{\mathsf{insert}}
\newcommand{\Set}{\mathsf{Set}}
\renewcommand{\tt}{\mathsf{tt}}

\newcommand{\Nat}{\mathsf{Nat}}
\newcommand{\zero}{\mathsf{zero}}
\newcommand{\suc}{\mathsf{suc}}
\newcommand{\Tel}{\mathsf{Tel}}
\newcommand{\TCons}{\triangleright}
\newcommand{\Rec}{\mathsf{Rec}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\yes}{\text{Yes}}
\newcommand{\yesst}{\text{Yes*}}
\newcommand{\no}{\text{No}}

\theoremstyle{remark}
\newtheorem{notation}{Notation}


%% ------------------------------------------------------------


\begin{document}

%% Title information
\title{Elaboration with First-Class Implicit Function Types}


%% Author with single affiliation.
\author{Andr{\'a}s Kov{\'a}cs}
\orcid{0000-0002-6375-9781}
\affiliation{
  \department{Department of Programming Languages and Compilers}
  \institution{E{\"o}tv{\"o}s Lor{\'a}nd University}
  \city{Budapest}
  \country{Hungary}
}
\email{kovacsandras@inf.elte.hu}


\begin{abstract}
Implicit functions are dependently typed functions, such that arguments are
provided (by default) by inference machinery instead of programmers of the
surface language. Implicit functions in Agda are an archetypal example. In the
Haskell language as implemented by the Glasgow Haskell Compiler (GHC),
polymorphic types are another example. Implicit function types are
\emph{first-class} if they are treated as any other type in the surface
language. This holds in Agda and partially holds in GHC. Inference and
elaboration in the presence of first-class implicit functions poses a challenge;
in the context of Haskell and ML-like languages, this has been dubbed
``impredicative instantiation'' or ``impredicative inference''. We propose a new
solution for elaborating first-class implicit functions, which is applicable
to full dependent type theories and compares favorably to prior solutions in
terms of power, generality and simplicity. We build atop Norell's
bidirectional elaboration algorithm for Agda, and we note that the key issue is
incomplete information about insertions of implicit abstractions and
applications. We make it possible to track and refine information related to
such insertions, by adding a function type to a core Martin-L\"of type theory,
which supports strict (definitional) currying. This allows us to represent
undetermined domain arities of implicit function types, and we can decide at any
point during elaboration whether implicit abstractions should be inserted.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code

%% Keywords
%% comma separated list
\keywords{impredicative polymorphism, type theory, elaboration, type inference}
\maketitle




\section{Introduction}
\label{sec:introduction}

Programmers and users of proof assistants do not like to write out obvious
things. Type inference and elaboration serve the purpose of filling in tedious
details, translating terse surface-level languages to explicit core
languages. Modern systems such as Agda have gotten quite adept at this
task. However, in practice, programmers still have to tell the compiler where to
try filling in details on its own.

\textbf{Implicit function types} are a common mechanism for conveying to the
compiler that particular function arguments should be inferred by default. In
Agda and Coq, one can use bracketed function domains for this purpose:
\begin{alignat*}{3}
  & id : \{A : \kw{Set}\}\to A \to A \hspace{5em} \kw{Definition}\,id\,\{A : \kw{Type}\}(x : A) := x.\\
  & id\,x = x
\end{alignat*}
In GHC, one can use $\kw{forall}$ to define implicit function
types\footnote{This notation requires language extensions
  $\mathsf{KindSignatures}$ and $\mathsf{RankNTypes}$; one could also write the type
  $a \to a$ and GHC would silently insert the quantification.}
\begin{alignat*}{3}
  & id :: \kw{forall}\,(a :: \kw{*}).\,a \to a\\
  & id\,x = x
\end{alignat*}
In all of the above cases, if we apply $id$ to an argument, the implicit type
argument is provided by elaboration. For example, in Agda, $id\,true$ is
elaborated to $id\,\{Bool\}\,true$, and analogously in GHC and Coq. In all three
systems, there is also a way to explicitly specify implicit arguments: in Agda
we may put arguments in brackets as we have seen, in Coq we can prefix a name
with $\kw{@}$ to make every implicit argument explicit, as in
$\kw{@}id\,bool\,true$, and in GHC we can enable the language extension
$\mathsf{TypeApplications}$ \cite{eisenberg2016visible} and write
$id\,\kw{@}Bool\,True$.

Implicit functions are \textbf{first-class} if they can be manipulated like any
other type. Coq is an example for a system where this is \emph{not} the case.
In Coq, the core language does not have an actual implicit function type,
instead, implicitness is tied to particular \emph{names}, and while we can write
$list\,(\kw{forall}\,\{A : \kw{Type}\},\,A\to A)$ for a list type with
polymorphic elements, the brackets here are simply ignored by Coq. For example,
Coq accepts the following definition:
\begin{alignat*}{3}
  & \kw{Definition}\,poly\,: \kw{forall}\,(f : \kw{forall}\,\{A : \kw{Type}\},\,A\to A),\,bool\,*\,nat :=\\
  & \quad \kw{fun}\,f \Rightarrow (f\,bool\,true,\,f\,nat\,0)
\end{alignat*}
This is a higher-rank polymorphic function which returns a pair. Note that $f$
is applied to two arguments, because the implicitness in $\kw{forall}\,\{A :
\kw{Type}\},\,A\to A$ is silently dropped.

In GHC Haskell, $\kw{forall}$ types are more flexible. We can write the
following, with $\kw{RankNTypes}$ enabled:
\begin{alignat*}{3}
  & poly :: (\kw{forall}\,a.\,a \to a) \to (Bool,\,Int) \\
  & poly\,f = (f\,True,\,f\,0)
\end{alignat*}
However, polymorphic types are only supported in function domains and as fields
of algebraic data constructors. We cannot instantiate an arbitrary type
parameter to a $\kw{forall}$, as in $[\kw{forall}\,a.\,a\to a]$ for a list type
with polymorphic elements. While this type is technically allowed by the
$\kw{ImpredicativeTypes}$ language extension, as of GHC 8.8 this extension is
deprecated and is not particularly usable in practice.

In Agda, implicit functions are truly a first-class notion, and we may have
$List\,(\{A : \kw{Set}\}\to A \to A)$ without issue. However, Agda's elaboration
still has limitations when it comes to handling implicit functions. Assume that
we have $[]$ for the empty list and $\blank::\blank$ for list extension,
and consider the following code:
\begin{alignat*}{3}
  & polyList : List\,(\{A : \kw{Set}\}\to A \to A)\\
  & polyList = (\lambda\,x \to x)\,::\,[]
\end{alignat*}
Agda 2.6.0.1 does not accept this. However, it does accept $polyList =
(\lambda\,\{A\}\,x \to x)\,::\,[]$. The issue is the following. Agda first
infers a type for $(\lambda\,x \to x)\,::\,[]$, then tries to unify the
inferred type with the given $List\,(\{A : \kw{Set}\}\to A \to A)$
annotation. However, when Agda elaborates $\lambda\,x \to x$, it does not yet
know anything about the element type of the list; it is an undetermined
unification variable. Hence, Agda does not know whether it should insert an
extra $\lambda\,\{A\}$ or not. If the element type is later found to be an
implicit function, then it should, otherwise it should not. To solve this
conundrum, Agda simply assumes that any unknown type is \emph{not} an implicit
function type, and elects to not insert a lambda. This assumption is often
correct, but sometimes --- as in the current case --- it is not.

There is significant literature on type inference in the presence of first-class
polymorphic types, mainly in relation to GHC and ML-like languages; see
e.g.\ \cite{leijen2008hmf,leijen2009flexible,serrano2018guarded,vytiniotis2006boxy}.
The above issue in Agda is a specific instance of the challenges described in
the mentioned works. Currently none of the above solutions are supported in
production compilers, for reasons of complexity, fragility and interaction with
other language features. A recent GHC development \cite{serrano2020a} offers a
solution which is relatively simple, and which is likely to land in an official
GHC release. However, none of these works support dependent types, which is a
key point in our work.

The solution presented in this paper is to gradually accummulate information
about implicit insertions, and to have a setup where insertions can be refined
and performed at any time after a particular expression is elaborated. In the
current example, our algorithm wraps $\lambda\,x\to x$ in an implicit lambda
with unknown arity, whose domain is later refined to be $A : \kw{Set}$ when the
inferred type is unified with the annotation.

\subsection{Contributions}
\begin{itemize}
  \item We propose an elaboration algorithm which translates from a small
    Agda-like surface language to a small Martin-L\"of type theory extended with
    implicit function types, telescopes and \emph{strictly curried function
      types} with telescope domain. We use these extensions to accumulate
    information about implicit insertions. Our algorithm is based on Norell's
    bidirectional elaborator for Agda \cite[Chapter~3]{norell07thesis}.
  \item
    In the System F fragment, the presented elaborator is comparable or superior
    to previous solutions for impredicative inference. However, it also supports
    full dependent type theory. Our inference is also global, i.e.\ it can
    consider the whole program and not just particular n-ary applications.
  \item We provide an executable implementation of the elaborator described in
    this paper.
  \item Our solution is simple: we implemented elaboration,
    evaluation and unification in about 670 lines of Haskell, of which 215 lines
    implement the novel enhancements on the top of Norell's basic elaborator.
  \item The target theory of elaboration serves as a general platform for
    elaborating implicit function types: our concrete elaborator is a relatively
    simple one, and there is room to further develop it.
\end{itemize}

\subsubsection{Note on terminology}

We prefer to avoid the term ``impredicative inference'' in order to avoid
confusion with impredicativity in type theory. The two notions sometimes
coincided historically, but currently they are largely orthogonal. In type
theory, impredicativity is a property of a universe, i.e.\ closure of a universe
under arbitrary products. In the type inference literature, impredicativity
means the ability to instantiate type variables and metavariables to polymorphic
types. In particular, we have that
\begin{itemize}
  \item Agda has type-theory-predicative universes, but implements
    type-inference-impredicative elaboration with first-class implicit function
    types.
  \item Coq has type-theory-impredicative $\kw{Prop}$ universe (and optionally
    also $\kw{Set}$), but implements type-inference-predicative elaboration,
    because of the lack of implicit function types.
  \item GHC is type-theory-impredicative with $\kw{RankNTypes}$ enabled and
    $\kw{ImpredicativeTypes}$ \emph{disabled}, as we have $(\kw{forall}\,(a :: *).\, a
    \to a) :: *$.
\end{itemize}

\section{Bidirectional Elaboration}
\label{sec:bidirectional_elaboration}

First, we present a variant of Norell's bidirectional elaborator
\cite[Chapter~3]{norell07thesis}. Compared to ibid.\ we make some extensions and
simplifications; what we end up with can be viewed as a toy version of the
actual Agda elaborator. In this section, we use it to build the backbone of our
algorithm and illustrate the key issues. We extend this elaborator in Section
\ref{sec:extending_elaboration}.
\begin{figure}[h]
\begin{alignat*}{4}
  t,\,u,\,v,\,A,\,B,\,C\, :&:=\quad  && x\hspace{8em}              & \text{variable}                 &  \\
                           & |       && (x : A)\to B               & \text{function type}            &  \\
                           & |       && \{x : A\}\to B             & \text{implicit function type}   &  \\
                           & |       && t\,u                       & \text{application}              &  \\
                           & |       && t\,\{u\}                   & \text{implicit application}     &  \\
                           & |       && \lambda\,x.\, t            & \text{lambda abstraction}       &  \\
                           & |       && \lambda\,\{x\}.\,t         & \text{implicit abstraction}     &  \\
                           & |       && \U                         & \text{universe}                 &  \\
                           & |       && \slet\,x : A = t\,\sin\, u & \text{let-definition}           &  \\
                           & |       && \_                         & \text{hole for inferred term}   &
\end{alignat*}
\caption{Syntax of the surface language.}
\label{fig:surface}
\end{figure}

\begin{figure}

\begin{alignat*}{2}
  & \boxed{\Theta\vdash}\hspace{6em}           && \text{\emph{metacontext formation}}\\
  & \boxed{\Theta|\Gamma\vdash}                && \text{\emph{context formation}}\\
  & \boxed{\Theta|\Gamma\vdash t : A}          && \text{\emph{typing}}\\
  & \boxed{\Theta|\Gamma\vdash t \equiv u : A} && \text{\emph{term equality}}
\end{alignat*}

\begin{mathpar}
  \inferrule*[lab=metacon/empty]
             {\\}
             {\emptycon \vdash}

  \inferrule*[lab=metacon/bind]
             {\Theta \vdash \\ \Theta \vdash A : \U}
             {\Theta, \alpha : A \vdash}

  \inferrule*[lab=con/empty]
             {\Theta \vdash}
             {\Theta|\emptycon \vdash}

  \inferrule*[lab=con/bind]
             {\Theta|\Gamma \vdash \\ \Theta|\Gamma \vdash A : \U}
             {\Theta|\Gamma,\,x : A \vdash}

  \inferrule*[lab=con/define]
             {\Theta|\Gamma \vdash \\ \Theta|\Gamma \vdash t : A}
             {\Theta|\Gamma,\,x : A = t \vdash}

  \inferrule*[lab=metavar]
             {\\}
             {\Theta_0,\,\alpha : A,\,\Theta_1|\Gamma \vdash \alpha : A}

  \inferrule*[lab=bound-var]
             {\\}
             {\Theta|\Gamma,\,x : A,\,\Delta \vdash x : A}

  \inferrule*[lab=defined-var]
             {\\}
             {\Theta|\Gamma,\,x : A = t,\,\Delta \vdash x : A}

  \inferrule*[lab=universe]
             {\\}
             {\Theta|\Gamma \vdash \U : \U}

  \inferrule*[lab=let]
             {\Theta|\Gamma\vdash t : A \\ \Theta|\Gamma,\,x : A = t \vdash u : B}
             {\Theta|\Gamma\vdash \slet\,x : A = t\,\sin\,u : B[x\mapsto t]}

  \inferrule*[lab=fun]
             {\Theta|\Gamma \vdash A : \U \\ \Theta|\Gamma,\,x : A \vdash B : \U}
             {\Theta|\Gamma \vdash (x : A) \to B : \U}

  \inferrule*[lab=implicit-fun]
             {\Theta|\Gamma \vdash A : \U \\ \Theta|\Gamma,\,x : A \vdash B : \U}
             {\Theta|\Gamma \vdash \{x : A\} \to B : \U}

  \inferrule*[lab=app]
             {\Theta|\Gamma \vdash t : (x : A) \to B \\ \Theta|\Gamma\vdash u : A}
             {\Theta|\Gamma \vdash t\,u : B[x \mapsto u]}

  \inferrule*[lab=implicit-app]
             {\Theta|\Gamma \vdash t : \{x : A\} \to B \\ \Theta|\Gamma\vdash u : A}
             {\Theta|\Gamma \vdash t\,\{u\} : B[x \mapsto u]}

  \inferrule*[lab=lam]
             {\Theta|\Gamma,\,x : A \vdash t : B}
             {\Theta|\Gamma \vdash \lambda\,x.\,t : (x : A) \to B}

  \inferrule*[lab=implicit-lam]
             {\Theta|\Gamma,\,x : A \vdash t : B}
             {\Theta|\Gamma \vdash \lambda\,\{x\}.\,t : \{x : A\} \to B}

  \inferrule*[lab=fun-$\beta$]
             {\Theta|\Gamma,\,x : A \vdash t : B \\\Theta|\Gamma\vdash u : A }
             {\Theta|\Gamma\vdash (\lambda\,x.\,t)\,u \equiv t[x\mapsto u] : B[x\mapsto u]}

  \inferrule*[lab=implicit-fun-$\beta$]
             {\Theta|\Gamma,\,x : A \vdash t : B \\\Theta|\Gamma\vdash u : A }
             {\Theta|\Gamma\vdash (\lambda\,\{x\}.\,t)\,\{u\} \equiv t[x\mapsto u] : B[x\mapsto u]}

  \inferrule*[lab=fun-$\eta$]
             {\Theta|\Gamma\vdash t : (x : A)\to B}
             {\Theta|\Gamma\vdash (\lambda\,x.\,t\,x) \equiv t : (x : A)\to B}

  \inferrule*[lab=implicit-fun-$\eta$]
             {\Theta|\Gamma\vdash t : \{x : A\}\to B}
             {\Theta|\Gamma\vdash (\lambda\,\{x\}.\,t\,\{x\}) \equiv t : \{x : A\}\to B}

  \inferrule*[lab=definition]
             {\\}
             {\Theta|\Gamma,\,x : A = t,\,\Delta \vdash x \equiv t : A}
\end{mathpar}
\caption{Selected rules of the core language.}
\label{fig:plaincore}
\end{figure}

\subsection{Surface syntax}
Figure \ref{fig:surface} shows the the possible constructs in the surface
language. We only have terms, as we have Russell-style universe in the core, and
we can conflate types and terms for convenience. The surface syntax does not
have semantics or any well-formedness relations attached; its sole purpose is to
serve as input to elaboration. Hence, the surface syntax can be also viewed as a
small untyped tactic language which is interpreted by the elaborator.

The syntactic constructs are the almost the same in the surface language as in
the core syntax. The difference is that $\_$ holes only appear in surface
syntax. The $\_$ can be used to request a term to be inferred by elaboration,
the same way as in Agda.  This can be used to give let-definitions without type
annotation, as in $\slet\,x : \_ = \U\,\sin\,x$.

\subsection{Core syntax}

Figure \ref{fig:plaincore} lists selected rules of the core language. We avoid a
fully formal presentation in this paper. Some notes on what is elided:
\begin{itemize}
  \item We use nameful notation and implicit weakening, i.e.\ whenever a term is
    well-formed in some context, it is assumed to be well-formed (as it is) in
    extended contexts. We also assume that any specifically mentioned name is
    fresh, e.g.\ when we write $\Theta,\,\alpha : A$, we assume that $\alpha$ is
    fresh in $\Theta$.  Formally, we would use de Bruijn indices for variables,
    and define variable renaming and parallel substitution by recursion on
    presyntax, e.g.\ as in \cite{schafer2015autosubst}.
  \item Fixing any $\Theta$ metacontext, parallel substitutions of bound and
    defined variables form morphisms of a category, where the identity
    substitution $\id$ maps each variable to itself and composition
    $\blank\circ\blank$ is given by pointwise substitution. The action of
    parallel substitution on terms is functorial, i.e.\ $t[\sigma][\delta]
    \equiv t[\sigma\circ\delta]$ and $t[\id] \equiv t$, and typing is stable
    under substitution.
  \item
    Definitional equality is understood to be a congruence and an equivalence relation,
    which is respected by substitution and typing.
  \item
    We elide a number of well-formedness assumptions in rules. For instance, whenever
    a context appears in a rule, it is assumed to be well-formed. Likewise, whenever
    we have $\Theta|\Gamma\vdash t : A$, we assume that $\Theta|\Gamma\vdash A : \U$.
\end{itemize}

From now on, we will only consider well-formed core syntax, and unless otherwise
mentioned, presented constructions on the core syntax respect definitional
equality.

Alternatively, one could present the syntax as a generalized algebraic theory
\cite{sterling2019algebraic} or a quotient inductive-inductive type
\cite{ttintt}, in which case we would get congruences and quotienting for free,
and we would also get a rich model theory for our syntax. However, it seems that
there are a number of possible choices for giving an algebraic presentation of
metacontexts, and existing works on algebraic presentations of dependent modal
contexts (e.g.\ \cite{birkedal2018modal}) do not precisely cover the current use
case. We leave this to future work, along with the investigation of elaboration
from an algebraic perspective.

Metacontexts are used to record metavariables which are created during
elaboration. In our case, metacontexts are simply a context prefix, and we have
variables pointing into it. This corresponds to a particularly simple variant of
\emph{crisp type theory} \cite{licata2018internal}, where we do not have modal
type operators or functions with crisp (``meta'') domain. The non-meta typing
context additionally supports \emph{defined variables}, which is used in the
typing rule for $\slet$-definitions, and we have that any defined variable is
equal to its definition. We mainly support this as a convenience feature in the
surface language.

The universe $\U$ is Russell-style, and we have the type-in-type rule.
This causes our core syntax to be non-total, and our elaboration algorithm to be
possibly non-terminating. We use type-in-type to simplify presentation, since
consistent universe setups are orthogonal to the focus of this work.

Function types only differ from each other in notation: implicit
functions have the same rules as ``explicit'' functions. The primary purpose of
implicit function types is to \emph{guide elaboration}: the elaborator will at
times compute a type and branch on whether it is an implicit function.

\emph{Notations.} We use Agda-like syntactic sugar both in the surface syntax and
in core syntax.
\begin{itemize}
  \item We use $A \to B$ to refer to non-dependent functions.
  \item We group domain types together in functions, and omit function arrows,
    as in $\{A\,B : \U\}(x : A) \to B \to A$.
  \item We group multiple $\lambda$-s, as in $\lambda\,\{A\}\,\{B\}\,x\,y.\,x$.
\end{itemize}
\begin{definition}[Spines]\label{def:spines}
  We use a spine notation for neutral terms. A spine is a list of
  terms, noted as $\overline{t}$, where terms may be wrapped in brackets to
  signal implicit application. For example, if $\overline{u} \equiv
  (\{A\},\,\{B\},\,x)$, then $t\,\overline{u}$ denotes $t\,\{A\}\,\{B\}\,x$.  In
  $t\,\overline{u}$, we call $t$ the \emph{head} of the neutral term. In
  particular, if $t$ is a metavariable, the neutral term is \emph{meta-headed}.
\end{definition}
\begin{example}
The core syntax is quite expressive as a programming language, thanks to
$\slet$-definitions\footnote{In dependent type theories, the $\slet$ rule is not
  derivable from function application, unlike in simple type theories.} and the
type-in-type rule which allows Church-encodings of a large class of inductive
types. For example, the following term computes a list of types by mapping:
\begin{alignat*}{3}
  & \slet\,List : \U\to\U\\
  & \hspace{1em}= \lambda\,A.\,(L : \U)\to(A\to L\to L)\to L\to L\,\,\sin\\
  & \slet\,map : \{A\,B : \U\}\to (A \to B) \to List\,A \to List\,B\\
  & \hspace{1em}=
  \lambda\,\{A\}\,\{B\}\,f\,as\,L\,cons\,nil.\,as\,L\,(\lambda\,a.\,cons\,(f\,a))\,nil\,\,\sin\\
  & map\,\{\U\}\,\{\U\}\,(\lambda\,A.\, A \to A)\,(\lambda\,L\,cons\,nil.\,cons\,\U\,(cons\,\U\,nil))
\end{alignat*}
\end{example}

\subsection{Metasubstitutions}\label{sec:metasubstitutions}

Before we can move on to the description of the elaborator, we need to specify
metasubstitutions. These are essentially just parallel substitutions of
metacontexts, and their purpose is to keep track of meta-operations (e.g.\ fresh
meta creation or solution of a meta).

\begin{itemize}
  \item
    A metasubstitution $\boxed{\theta : \Theta_0 \To \Theta_1}$ assigns to each variable
    in $\Theta_1$ a term in $\Theta_0$ , hence it is represented as a list of
    terms $(\alpha_1 \mapsto t_1,\,...\,\,\alpha_i \mapsto t_i)$.
  \item
    We define the action of a metasubstitution on contexts and terms by
    recursion; we notate action on contexts as $\Gamma[\theta]$ and action on
    terms as $t[\theta]$. We remark that there is no abstraction for
    metavariables in the core syntax, so we do not have to handle variable
    capture (or index shifting).
\end{itemize}
The following are admissible:
\begin{mathpar}
  \inferrule*[lab=metasub/empty]
             {\Theta\vdash}
             {() : \Theta\To \emptycon}

  \inferrule*[lab=metasub/extended]
             {\theta : \Theta_0\To\Theta_1 \\ \Theta_0|\emptycon\vdash t : A[\theta]}
             {(\theta,\, \alpha\mapsto t) : \Theta_0\To(\Theta_1,\,\alpha : A)}

  \inferrule*[lab=metasub/con-action]
             {\theta : \Theta_0\To\Theta_1 \\ \Theta_1|\Gamma\vdash}
             {\Theta_0|\Gamma[\theta]\vdash}

  \inferrule*[lab=metasub/tm-action]
             {\theta : \Theta_0\To\Theta_1 \\ \Theta_1|\Gamma\vdash t : A}
             {\Theta_0|\Gamma[\theta] \vdash t[\theta] : A[\theta]}

  \inferrule*[lab=metasub/identity]
             {\\}
             {\id : \Theta\To\Theta}

  \inferrule*[lab=metasub/composition]
             {\theta_{0} : \Theta_1 \To \Theta_2 \\ \theta_1 : \Theta_0 \To \Theta_1}
             {\theta_0 \circ \theta_1 : \Theta_0 \To \Theta_2}

  \inferrule*[lab=metasub/weakening]
             {\\}
             {\p : (\Theta,\,x : A) \To \Theta}
\end{mathpar}

%% \end{mathpar}
%% \begin{mathpar}

The identity substitution $\id$ maps each variable to itself. Composition is
given by pointwise term substitution, $\id$ and $\blank\circ\blank$ yields a
category, and the action of metasubstitution on contexts and terms is
functorial. The weakening substitution $\p$ (the naming comes from
categories-with-families terminology \cite{dybjer1995internal}) can be defined
as dropping the last entry from $\id : (\Theta,\,x : A) \To (\Theta,\,x : A)$.

\subsection{Fresh Metavariables}

Using \emph{contextual metavariables} is a standard practice in the
implementation of dependently typed languages. This means that every ``hole'' in
the surface language is represented as an unknown function which abstracts over
all bound variables in the scope of a hole. Unlike \cite{nanevski2008contextual}
and similarly to \cite{gundry2013type}, we do not have a first-class notion of
contextual types, and instead reuse the standard dependent function type to
abstract over enclosing contexts.

\begin{definition}[Closing type]\label{def:closingtype} For each $\Theta|\Gamma \vdash A : \U$, we define
$\Gamma \To A$ by recursion on $\Gamma$, such that $\Theta|\emptycon\vdash
  \Gamma \To A : \U$.
  \begin{alignat*}{3}
    &((\Gamma,\, x : A) \To B)     && :\equiv (\Gamma \To ((x : A) \to B))\\
    &((\Gamma,\, x : A = t) \To B) && :\equiv (\Gamma \To B[x\mapsto t])\\
    &(\emptycon \To B)             && :\equiv B
  \end{alignat*}
\end{definition}

\begin{definition}[Contextualization]\label{def:contextualization}
For each $\Theta|\Gamma \vdash t : \Gamma\To A$, we define the spine
$\overline{\mathsf{vars}_{\Gamma}}$ such that that $\Theta|\Gamma\vdash
t\,\overline{\mathsf{vars}_{\Gamma}} : A$, which is $t$ applied to all bound
variables in $\Gamma$.
  \begin{alignat*}{3}
    &(t\,\overline{\mathsf{vars}_{\Gamma,\,x:A}})   && :\equiv (t\,\overline{\mathsf{vars}_{\Gamma}})\,x \\
    &(t\,\overline{\mathsf{vars}_{\Gamma,\,x:A=t}}) && :\equiv (t\,\overline{\mathsf{vars}_{\Gamma}}) \\
    &(t\,\overline{\mathsf{vars}_{\emptycon}})     && :\equiv t
  \end{alignat*}
\end{definition}

\begin{example} If we have $\Gamma \equiv (\emptycon,\,A : \U,\,B : A \to \U)$, then
$(\Gamma \To \U) \equiv ((A : \U)(B : A \to \U) \to \U)$ and
$t\,\overline{\mathsf{vars}_{\Gamma}} \equiv t\,A\,B$.
\end{example}

\begin{definition}[Fresh meta creation]\label{def:freshmeta} We specify $\freshMeta{\Theta}{\Gamma}{A}$ as follows:
  \begin{mathpar}
  \inferrule*{\Theta|\Gamma\vdash A : \U}
             {\freshMeta{\Theta}{\Gamma}{A} \in \{(\Theta',\,\theta,\,t)\,|\,(\theta : \Theta' \To \Theta)\,\land\,(\Theta'|\Gamma[\theta]\vdash t : A[\theta])\}}
  \end{mathpar}
The definition $\freshMeta{\Theta}{\Gamma}{A} :\equiv ((\Theta,\,\alpha:\Gamma\To
A),\,\p,\,\alpha\,\overline{\mathsf{vars}_{\Gamma}})$, where $\alpha$ is fresh in
$\Theta$, satisfies this specification. We extend $\Theta$ with a fresh meta,
which has the closing type $\Gamma \To A$. The $\p$ weakening relates the new
metacontext to the old one, by ``dropping'' the new entry. Lastly,
$\alpha\,\overline{\mathsf{vars}_{\Gamma}}$ is the fresh meta
applied to all bound variables.
\end{definition}

\subsection{Unification}

We assume that there is a unification procedure, which returns a unifying
metasubstitution on success. We only have \emph{homogeneous} unification,
i.e.\ the two terms to be unified must have the same type. The specification is
as follows:
\begin{mathpar}
\inferrule*{\Theta|\Gamma\vdash t : A \\ \Theta|\Gamma\vdash u : A}
           {\unify\,t\,u \in \{(\Theta',\,\theta)\,|\,(\theta : \Theta' \To \Theta)
             \,\land\,(\Theta'|\Gamma[\theta]\vdash
             t[\theta] \equiv u[\theta] : A[\theta])\}\,\cup\,\{\fail\}}
\end{mathpar}
For a simple example, assuming $\Theta :\equiv (\emptycon,\,\alpha : \U,\, \beta
: \U)$, $\unify\,\alpha\,(\beta\to\beta)$ yields $\Theta'
:\equiv (\emptycon,\,\beta : \U)$ and the substitution $\theta :\equiv (\alpha
\mapsto (\beta\to\beta),\,\beta\mapsto \beta)$, where $\theta : \Theta' \To
\Theta$.

Here, we do not require that unification returns most general unifiers, nor do
we go into the details of how unification is implemented. Gundry describes
unification in detail in \cite[Chapter~4]{gundry2013type} for a similar syntax,
with a similar (though more featureful) setup for metacontexts. See also
\cite{abel2011higher} for a reference on unification. Note that our unification
algorithm does not support \emph{constraint postponing}, as we have not talked
about constraints at all. In our concrete prototype implementation, unification
supports basic pattern unification and metavariable pruning.

\subsection{Elaboration}

Elaboration consists of two (partial) functions, checking and inferring, which
are defined by mutual induction on surface syntax. We also have implicit
argument insertion as a helper function, which is defined by recursion on core
types, and which is used as a post-processing step after inference. First, about
the used notations:
\begin{itemize}
  \item We use a Haskell-like monadic pseudocode notation, where the side effect is
    failure via $\fail$.
  \item We use pattern matching notation on core terms; e.g.\ we may match on
    whether a type is a function type. This assumes an evaluation/normalization
    procedure on core terms; but note that we already assume this in
    unification.
  \item We abbreviate $\theta_1 \circ \theta_2$ as $\theta_{12}$, $\theta_1
    \circ \theta_2 \circ \theta_3$ as $\theta_{123}$ and analogously in other
    cases. We do this to reduce the visual noise caused by threading composed
    metasubstitutions everywhere in the elaboration algorithm.
\end{itemize}
We present the specifications and definitions below, then we describe them in order.
\begin{mathpar}
\inferrule*[lab=insert]{\Theta_0|\Gamma\vdash \\ (\Theta,\,\theta,\,t,\,A) \in
             \{(\Theta,\,\theta,\,t,\,A)\,|\,(\theta : \Theta \To \Theta_0)\,\land\,
               (\Theta|\Gamma[\theta]\vdash t : A)\}\cup\{\fail\}}
           {\einsert(\Theta,\,\theta,\,t,\,A) \in \{(\Theta,\,\theta,\,t,\,A)\,|\,
             (\theta : \Theta \To \Theta_0)\,\land\,(\Theta|\Gamma[\theta]\vdash t : A)\}\cup\{\fail\}}
\end{mathpar}
\begin{alignat*}{3}
  &\einsert\,(\Theta_1,\,\theta_1,\,t,\,\{x : A\}\to B) && :\equiv\hspace{0.5em}&&
       \slet\,(\Theta_2,\,\theta_2,\,u) = \freshMeta{\Theta}{\Gamma}{A}\\
  & && &&\sin\,\einsert\,(\Theta_2,\,\theta_{12},\,(t[\theta_2])\,\{u\},\,B[\theta_2][x\mapsto u])\\
  &\einsert\,(\Theta_1,\,\theta_1,\,t,\,A) && :\equiv&& \ereturn\,(\Theta_1,\,\theta_1,\,t,\,A)\\
  &\einsert\,\fail && :\equiv && \fail
\end{alignat*}
\pagebreak
\begingroup
\allowdisplaybreaks
\begin{mathpar}
\inferrule*[lab=check]
           {\text{$t$ is a surface expression} \\ \Theta|\Gamma \vdash A : \U}
           {\echeck{t}{\Theta}{\Gamma}{A} \in
             \{(\Theta',\,\theta,\,t')\,|\,(\theta : \Theta' \To \Theta)\,\land\,
               (\Theta'|\Gamma[\theta]\vdash t' : A[\theta])\}\cup\{\fail\}
           }

\inferrule*[lab=infer]
           {\text{$t$ is a surface expression} \\ \Theta|\Gamma \vdash}
           {\einfer{t}{\Theta}{\Gamma}\,\in
             \{(\Theta',\,\theta,\,t',\,A)\,|\,(\theta : \Theta' \To \Theta)\,\land\,
               (\Theta'|\Gamma[\theta]\vdash t' : A)\}\cup\{\fail\}
           }
\end{mathpar}
\begin{alignat*}{3}
  &\echeck{\lambda\,x.\,t}{\Theta}{\Gamma}{((x : A)\to B)} :\equiv \edo \\
  &\quad(\Theta',\,\theta,\,t') \leftarrow \echeck{t}{\Theta}{\Gamma,\,x:A} B\\
  &\quad\ereturn\,(\Theta',\,\theta,\,\lambda\,x.\,t')\\
  &\echeck{\lambda\,\{x\}.\,t}{\Theta}{\Gamma}{(\{x : A\}\to B)} :\equiv \edo \\
  &\quad(\Theta',\,\theta,\,t') \leftarrow \echeck{t}{\Theta}{\Gamma,\,x:A} B\\
  &\quad\ereturn\,(\Theta',\,\theta,\,\lambda\,\{x\}.\,t')\\
  &\echeck{t}{\Theta}{\Gamma}{(\{x : A\}\to B)} : \equiv \edo \\
  &\quad (\Theta',\,\theta,\,t') \leftarrow \echeck{t}{\Theta}{\Gamma,\,x:A} B\\
  &\quad (\Theta',\,\theta,\,\lambda\,\{x\}.\,t')\\
  &\echeck{\slet\,x:A=t\,\sin\,u}{\Theta_0}{\Gamma}{B} :\equiv \edo \\
  &\quad(\Theta_1,\,\theta_1,\,A') \leftarrow \echeck{A}{\Theta_0}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,t') \leftarrow \echeck{t}
                {\Theta_1}{\Gamma[\theta_1]}{A'}\\
  &\quad(\Theta_3,\,\theta_3,\,u') \leftarrow
                \echeck{u}{\Theta_2}{\Gamma[\theta_{12}]}{(B[\theta_{12}])}\\
  &\quad\ereturn\,(\Theta_3,\,\theta_{123},\,\slet\,x:A'[\theta_{23}]=t'[\theta_3]\,\sin\,u')\\
  &\echeck{\_}{\Theta}{\Gamma}{A} :\equiv \edo \\
  &\quad\ereturn\,(\freshMeta{\Theta}{\Gamma}{A})\\
  &\echeck{t}{\Theta_0}{\Gamma}{A} :\equiv \edo \\
  &\quad(\Theta_1,\,\theta_1,\,t',\,B) \leftarrow \einsert\,(\einfer{t}{\Theta_0}{\Gamma})\\
  &\quad(\Theta_2,\,\theta_2) \leftarrow \unify\,(A[\theta_1])\,B\\
  &\quad\ereturn\,(\Theta_2,\,\theta_{12},\,t'[\theta_2])\\\\
  &\einfer{x}{\Theta}{\Gamma} :\equiv \edo\\
  &\quad\eif\,(\Gamma = (\Gamma_0,\,x : A,\,\Gamma_1))\,\lor\,(\Gamma = (\Gamma_0,\,x : A = t,\,\Gamma_1)) \\
  &\qquad   \ethen\, \ereturn\,(\Theta,\,\id,\,x,\,A)\\
  &\qquad   \eelse\, \fail\\
  &\einfer{\U}{\Theta}{\Gamma} :\equiv \edo\\
  &\quad\ereturn\,(\Theta,\,\id,\,\U,\,\U)\\
  &\einfer{(x : A)\to B}{\Theta_0}{\Gamma} :\equiv \edo\\
  &\quad(\Theta_1,\,\theta_1,\,A') \leftarrow \echeck{A}{\Theta_1}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,B') \leftarrow
                 \echeck{B}{\Theta_2}{\Gamma[\theta_1],\,x : A'}{\U}\\
  &\quad\ereturn\,(\Theta_2,\,\theta_{12},
                 \,((x : A'[\theta_2])\to B'),\,\U)\\
  &\einfer{\{x : A\}\to B}{\Theta_0}{\Gamma} :\equiv \edo\\
  &\quad(\Theta_1,\,\theta_1,\,A') \leftarrow \echeck{A}{\Theta_1}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,B') \leftarrow
                 \echeck{B}{\Theta_2}{\Gamma[\theta_1],\,x : A'}{\U}\\
  &\quad\ereturn\,(\Theta_2,\,\theta_{12},
                 \,(\{x : A'[\theta_2]\}\to B'),\,\U)\\
  &\einfer{\lambda\,x.\,t}{\Theta_0}{\Gamma} :\equiv \edo \\
  &\quad \slet\, (\Theta_1,\,\theta_1,\,A) = \freshMeta{\Theta_0}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,t',\,B)\leftarrow \einsert\,(\einfer{t}{\Theta_1}{\Gamma[\theta_1],\,x:A})\\
  &\quad\ereturn\,(\Theta_2,\,\theta_{12},\,\lambda\,x.\,t',\,(x : A[\theta_2])\to B)\\
  &\einfer{\lambda\,\{x\}.\,t}{\Theta_0}{\Gamma} :\equiv \edo \\
  &\quad \slet\,(\Theta_1,\,\theta_1,\,A) = \freshMeta{\Theta_0}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,t',\,B)\leftarrow \einsert\,(\einfer{t}{\Theta_1}{\Gamma[\theta_1],\,x:A})\\
  &\quad\ereturn\,(\Theta_2,\,\theta_{12},\,\lambda\,\{x\}.\,t',\,\{x : A[\theta_2]\}\to B)\\
  &\einfer{t\,u}{\Theta_0}{\Gamma} :\equiv \edo \\
  &\quad (\Theta_1,\,\theta_1,\,t',\,A) \leftarrow \einsert\,(\einfer{t}{\Theta_0}{\Gamma})\\
  &\quad \slet\,(\Theta_2,\,\theta_2,A_0) = \freshMeta{\Theta_1}{\Gamma[\theta_1]}{\U}\\
  &\quad \slet\,(\Theta_3,\,\theta_3,A_1) = \freshMeta{\Theta_2}{\Gamma[\theta_{12}],\,x:A_0}{\U}\\
  &\quad (\Theta_4,\,\theta_4) \leftarrow \unify\,(A[\theta_{23}])\,((x : A_0[\theta_3])\to A_1)\\
  &\quad (\Theta_5,\,\theta_5,\,u') \leftarrow \echeck{u}{\Theta_4}{\Gamma[\theta_{1234}]}{(A_0[\theta_{34}])}\\
  &\quad \ereturn\,(\Theta_5,\,\theta_{12345},\,(t'[\theta_{2345}])\,u',\,A_1[\theta_{45}][x\mapsto u'])\\
  &\einfer{t\,\{u\}}{\Theta_0}{\Gamma} :\equiv \edo \\
  &\quad (\Theta_1,\,\theta_1,\,t',\,A) \leftarrow \einfer{t}{\Theta_0}{\Gamma}\\
  &\quad \slet\,(\Theta_2,\,\theta_2,A_0) = \freshMeta{\Theta_1}{\Gamma[\theta_1]}{\U}\\
  &\quad \slet\,(\Theta_3,\,\theta_3,A_1) = \freshMeta{\Theta_2}{\Gamma[\theta_{12}],\,x:A_0}{\U}\\
  &\quad (\Theta_4,\,\theta_4) \leftarrow \unify\,(A[\theta_{23}])\,(\{x : A_0[\theta_3]\}\to A_1)\\
  &\quad (\Theta_5,\,\theta_5,\,u') \leftarrow \echeck{u}{\Theta_4}{\Gamma[\theta_{1234}]}{(A_0[\theta_{34}])}\\
  &\quad \ereturn\,(\Theta_5,\,\theta_{12345},\,(t'[\theta_{2345}])\,\{u'\},\,A_1[\theta_{45}][x\mapsto u'])\\
  &\einfer{\slet\,x:A=t\,\sin\,u}{\Theta_0}{\Gamma} :\equiv \edo \\
  &\quad(\Theta_1,\,\theta_1,\,A') \leftarrow \echeck{A}{\Theta_0}{\Gamma}{\U}\\
  &\quad(\Theta_2,\,\theta_2,\,t') \leftarrow \echeck{t}
                {\Theta_1}{\Gamma[\theta_1]}{A'}\\
  &\quad(\Theta_3,\,\theta_3,\,u',\,B) \leftarrow
                \einfer{u}{\Theta_2}{\Gamma[\theta_{12}]}\\
  &\quad\ereturn\,(\Theta_3,\,\theta_{123},\,(\slet\,x:A'[\theta_{23}]=t'[\theta_3]\,\sin\,u'),\,B)\\
  &\einfer{\_}{\Theta}{\Gamma} :\equiv \edo \\
  &\quad \slet\,(\Theta',\,\theta,\,A) = \freshMeta{\Theta}{\Gamma}{\U}\\
  &\quad \ereturn\,(\freshMeta{\Theta'}{\Gamma[\theta]}{A})\\
\end{alignat*}
\endgroup
\subsubsection{Implicit argument insertion}
This inserts implicit applications around a core term. For example, if we have a
defined name $id$ with type $\{A : \U\}\to A \to A$ in a surface program, we
usually want to expand $id$ to $id\,\{\alpha\}$, where $\alpha$ is a fresh
metavariable. We define $\einsert$ to take as input the output of
$\einferblank$, so that it is more convenient to use as an optional
post-processing step after inference.

\subsubsection{Checking}
The first two clauses are checking $\lambda$-s, where the expected type exactly
matches the $\lambda$ binders. Hence, we simply check under binders with
$\echeck{t}{\Theta}{(\Gamma,\,x:A)} B$, and wrap the resulting term in the
appropriate (implicit or explicit) $\lambda$.

The third clause for $\echeck{t}{\Theta}{\Gamma}{(\{x : A\}\to B)}$ is more
interesting. Here, we are checking a surface term which is \emph{not} a
$\lambda$ (this follows from our top-down pattern matching notation), with an
implicit function expected type. Here, we check $t$ in the extended $\Gamma,\,x
: A$ context, and we insert a new implicit $\lambda$ in the elaboration
output. This is the only point where implicit $\lambda$-s are introduced by
elaboration. Practically, this rule is commonly useful whenever we have a
higher-order function where some arguments have implicit function type. For
example, in the surface syntax, assume natural numbers, and an induction
principle for them:
\[
NatInd : \{P : Nat \to \U\}\to \,P\,\zero \to (\{n : Nat\}\to P\,n \to
         P\,(\suc\,n)) \to (n : Nat) \to P\,n
\]
Then, define addition using induction:
\begin{alignat*}{1}
& \slet\, NatPlus : Nat \to Nat \to Nat\\
& \qquad = NatInd\,(\lambda\,n.\,\Nat\to Nat)\,(\lambda\,m.\,m)\,
                        (\lambda\,f\,m.\,\suc\,(f\,m))\,\,\sin\,\,...
\end{alignat*}
When the above is elaborated, the $\lambda\,f\,m.\,\suc\,(f\,m)$ function is
checked with the expected type $\{n : Nat\}\to (Nat \to Nat) \to (Nat \to Nat)$,
and the elaboration output is $\lambda\,\{n\}\,f\,m.\,\suc\,(f\,m)$.  Hence, in
this case we do not have to write implicit $\lambda$ in the surface syntax.

For $\echeck{\slet\,x:A=t\,\sin\,u}{\Theta_0}{\Gamma}{B}$, we simply let
checking fall through. For $\echeck{\_}{\Theta}{\Gamma}{A}$, we return a fresh
metavariable with the expected type. In any other
$\echeck{t}{\Theta_0}{\Gamma}{A}$ case, we have a \emph{change of direction}: we
infer a type for $t$ (with implicit insertions) then unify the expected and
inferred types.

\subsubsection{Inferring}

For $\einfer{x}{\Theta}{\Gamma}$, we look up the type of $x$ in $\Gamma$. In the
case of $\U$, we always succeed and infer $\U$ as type. In the cases for
function types, we check that the domains and codomains have type $\U$.  For
$\lambda$-s, we create a fresh meta for the domain type (since our surface
$\lambda$-s are not annotated), and infer types for the bodies.

The $\einfer{t\,u}{\Theta_0}{\Gamma}$ and $\einfer{t\,\{u\}}{\Theta_0}{\Gamma}$
cases are again interesting. Here, we first infer a type for $t$, then refine
the type to a function type, and lastly check the argument with the domain
type. Note the difference between the explicit and implicit case. In the former
case we use $\einsert\,(\einfer{t}{\Theta_0}{\Gamma})$, which inserts implicit
applications. In the latter case we do no insertion. This ensures that implicit
applications in the surface syntax behave similarly as in Agda. For example,
given $id : \{A : \U\}\to A\to A$ in scope, we elaborate $id\,\U$ as follows:
\begin{enumerate}
  \item The expression is an explicit application, so we infer $\id$ and insert implicit
        arguments, returning $id\,\{\alpha\}$, where $\alpha$ is a fresh meta.
  \item We check that $\U$ has type $\alpha$. Here we immediately change direction,
        inferring $\U$ as type for $\U$ and unifying $\alpha$ with $\U$.
  \item Hence, the resulting output is $id\,\{\U\}\,\U$.
\end{enumerate}
On the other hand, we elaborate $id\,\{\U\}$ as follows:
\begin{enumerate}
  \item This is an implicit application, so we infer a type for $id$ without inserting
    implicit arguments. This yields the inferred type $\{A : \U\}\to A\to A$, and
    we check that $\U$ has type $\U$.
  \item We change direction and infer $\U$ as type for $\U$, and successfully
    unify $\U$ with $\U$.
\end{enumerate}
It would be more efficient (and also allow more user-friendly error
messages) to not immediately refine the inferred type of $t$ to a function type,
but rather match on the inferred type, and only perform refining when the type
is meta-headed. We present the unoptimized version here for the sake of brevity.

In the case of $\slet$, inference again just falls through, and we infer a type
for the $\slet$ body. For $\einfer{\_}{ins}{\Theta}{\Gamma}$, we create a fresh meta
for the type of the hole, and another fresh meta for the hole itself.

\subsubsection{Starting and finishing elaboration} Given a surface term $t$,
we initiate elaboration by computing $\einfer{t}{\emptycon}{\emptycon}$. If this
succeeds, we get a $(\Theta,\,\theta,t')$ result. Elaboration is successful overall
if $\Theta = \emptycon$ and $\theta = \id$, i.e.\ no unsolved metas remain.

\subsubsection{Properties of elaboration}
First, elaboration is \emph{sound} in the sense that it only produces well-formed
output.

\begin{theorem}[Soundness] The definitions of $\echeckblank$ and
$\einferblank$ conform to the \LabTirName{check} and \LabTirName{infer}
  specifications. This follows by induction on surface syntax, while also
  relying on the properties of substitution, metasubstitution,
  $\mathsf{insert}$, $\unify$ and $\mathsf{freshMeta}$. \qed
\end{theorem}

We remark that this notion of soundness is only a ``sanity'' or well-typing
statement for elaboration. In fact, we could define elaboration as a constantly
failing partial function, and it would also conform to the specification. The
right way to view this, is that $\echeckblank$ and $\einferblank$ together
with their specification constitute the semantics of surface syntax. We do not
give any other semantics to the surface syntax, nor does it support any other
operation.

We do not present any \emph{completeness} result for elaboration in this
paper. For an example of what this would entail, in \cite{dunfield2013complete}
completeness means that whenever there is a way to fill in missing details in
the surface syntax, algorithmic typechecking \emph{always} finds it. In
ibid.\ this means figuring out domain types for $\lambda$-s and inserting all
implicit applications. However, our elaborator targets a far stronger theory,
and it is beyond our reach to succinctly characterize which annotations are
inferable in the surface language, and we do not know of any prior work which
accomplishes this for a comparably strong elaborator. Our experience in Agda is
that it is not tractable in general to figure out which arguments are inferable,
by looking at function types, and often we have to run elaboration to see what
works.

We can still say something about the behavior of our elaborator. For this, we
consider a translation from core terms to surface terms, the evident forgetful
translation, which maps core terms to surface counterparts. Now, this is an
``evil'' construction on core terms, since it does not preserve definitional
equality, but we shall only use this evil notion in the following statement.

\begin{theorem}[Conservativity]\label{thm:conservativity}
Elaboration is conservative over the surface syntax, in the sense that for any
surface term $t$, if checking or inference outputs $t'$, then the forgetful
translation of $t'$ differs from $t$ only by
  \begin{itemize}
    \item Having all $\_$ holes filled with core expressions.
    \item Having extra implicit $\lambda$-s and implicit applications inserted.
  \end{itemize}
This follows by straightforward induction on surface syntax. \qed
\end{theorem}

\emph{Remark.} It is \emph{not} the case that for every term in the core syntax
there exists a surface term which elaborates to it. The main reason is that
$\lambda$-s in the surface syntax are not annotated, and it is easy to find core
$\lambda$ expressions with uninferable domain types. We skip optional surface
$\lambda$ domain type annotations for the sake of brevity.

\subsubsection{Omitted features}

\begin{itemize}
  \item \emph{Let-generalization}. This is an open research topic in settings
    with dependent types, and we make no attempt at covering it. See
    \cite{eisenberg2016dependent} for a treatment in a proposed dependent
    version of Haskell.
  \item \emph{Polymorphic subtyping}. In some prior works, e.g.\ in
    \cite{dunfield2013complete, vytiniotis2008fph}, there is a subtyping
    relation arising along instantiations of polymorphic types. In GHC 8
    polymorphic subtyping is implemented for function types only. Polymorphic
    subtyping complicates type inference, and to our knowledge it has not been
    implemented in any dependently typed setting. We also believe that it is
    undesirable in dependent settings, because elaboration of subtyping must
    insert coercions which significantly change the intensional character of
    programs. For example, if we have covariant list types, then coercing $t :
    List\,(\{A : \U\}\to A \to A)$ to $t : List\,(Bool \to Bool)$ requires
    mapping over $t$ and inserting implicit applications to $Bool$ for each list
    element. In System F, all such coercions are erasible, since types are
    computationally irrelevant, but in our core syntax we have implicit
    functions with arbitrary (relevant) domains. In GHC, subtyping coercions for
    functions change operational semantics; this is a reason for abandoning
    subtyping in recent developments of impredicative inference for GHC
    \cite{serrano2020a}.
\end{itemize}

\section{Issues with First-Class Implicit Functions}
\label{sec:issues}

%% \begin{alignat*}{3}
%%   & List && : \U \to \U \\
%%   & nil  && : \{A : \U\} \to List\,A\\
%%   & cons && : \{A : \U\} \to A \to List\,A
%% \end{alignat*}

We revisit now the $polyList$ example from Section \ref{sec:introduction}. We
assume the following:
\[
List : \U \to \U \qquad nil : \{A : \U\} \to List\,A \qquad
cons : \{A : \U\} \to A \to List\,A
\]
In the following, we present a trace of checking $cons\,(\lambda\,x.\,x)\,nil$
at type $List\,(\{A : \U\}\to A \to A)$. We omit context and metacontext
parameters everywhere, and notate recursive calls by indentation. We also omit
some checking, inference, implicit insertion and unification calls which are not
essential for illustration.
\begin{alignat*}{3}
  & \scriptstyle{0 }\qquad\qquad && \echeckt{cons\,(\lambda\,x.\,x)\,nil}{(List\,(\{A : \U\}\to A \to A))}
     \hspace{14em}\\
  & \scriptstyle{1 }  && \quad \einfert{cons\,(\lambda\,x.\,x)\,nil} \\
  & \scriptstyle{2 }  && \qquad \einfert{cons\,(\lambda\,x.\,x)}\\
  & \scriptstyle{3 }  && \qquad\quad \einfert{cons}\\
  & \scriptstyle{4 }  && \qquad\quad = cons\,\{\alpha_0\}\,:\,\alpha_0 \to List\,\alpha_0 \to List\,\alpha_0\\
  & \scriptstyle{5 }  && \qquad\quad \echeckt{\lambda\,x.\,x}{\alpha_0}\\
  & \scriptstyle{6 }  && \qquad\quad = \lambda\,x.\,x\\
  & \scriptstyle{7 }  && \qquad = cons\,\{\alpha_1 \to \alpha_1\}\,(\lambda\,x.\,x) : List\,(\alpha_1\to\alpha_1) \to List\,(\alpha_1\to\alpha_1)\\
  & \scriptstyle{8 }  && \qquad \echeckt{nil}{(List\,(\alpha_1\to\alpha_1))}\\
  & \scriptstyle{9 }  && \qquad = nil\,\{\alpha_1\to\alpha_1\}\\
  & \scriptstyle{10} && \quad = cons\,\{\alpha_1 \to \alpha_1\}\,(\lambda\,x.\,x)\,(nil\,\{\alpha_1\to\alpha_1\}): List\,(\alpha_1\to\alpha_1)\\
  & \scriptstyle{11} && \quad \unify\,(List\,(\{A : \U\}\to A \to A))\,(List\,(\alpha_1\to\alpha_1))\\
  & \scriptstyle{12} && \qquad \unify\,(\{A : \U\}\to A \to A)\,(\alpha_1\to\alpha_1)\\
  & \scriptstyle{13} && \qquad = \fail
\end{alignat*}

Above, we first infer $cons\,(\lambda\,x.\,x)\,nil$, which inserts implicit
applications to fresh metas in $cons$ and $nil$, and returns $cons\,\{\alpha_1
\to \alpha_1\}\,(\lambda\,x.\,x)\,(nil\,\{\alpha_1\to\alpha_1\}) :
List\,(\alpha_1\to\alpha_1)$. Here, the $\alpha_0$ meta is refined to $\alpha_1
\to \alpha_1$ when we check $\lambda\,x.\,x$. In the end, we need to unify the
expected and inferred types, which fails, since we have an implicit function
type on one side and an explicit function on the other side.

Why does this fail? The culprit is line $\scriptstyle{5}$, where we call
$\echeckt{\lambda\,x.\,x}{\alpha_0}$. At this point, the checking type is not an
implicit function type (it is a meta), so we do not insert an implicit
$\lambda$. At the heart of the issue is that elaboration makes insertion choices
based on core types.
\begin{enumerate}
\item $\echeck{t}{\Theta}{\Gamma}{A}$ can insert a $\lambda$ only if $A$ is an implicit function type.
\item $\einsert\,(\Theta,\,\theta,\,t,\,A)$ inserts an application only if $A$ is an implicit function type.
\end{enumerate}
In both of these cases, if $A$ is of the form $\alpha\,\overline{u}$
(i.e.\ meta-headed), then it is possible that $\alpha$ is later refined to an
implicit function, but at that point we have already missed our shot at implicit
insertion.

There is a potential naive solution: just \emph{postpone} checking a term until
the shape of the checking type is known for sure. This was included as part of a
proposed solution for smarter $\lambda$-insertions in
\cite{johansson2015eliminating}. This means that checking with a meta-headed
type returns a ``guarded constant'' \cite[Chapter~3]{norell07thesis}, an opaque
stand-in which only computes to an actual core term when the checking type
becomes known. In practice, this solution has a painful drawback: \emph{we get
  no information at all from checked terms before the guard is unblocked}.

For an example for unexpected behavior with this solution, let us assume $Bool :
\U$ and $true : Bool$, and try to infer type for the surface term $\slet\,x : \_
= true\,\sin\, x$. We first insert a fresh meta $\alpha$ for the hole, and then
check $true$ with $\alpha$. We postpone this checking, returning a guarded
constant, and then infer a type for $x$, which is $\alpha$. Hence, this small
example yields an unsolved meta and a guarded constant in the output.

Now, this particular example can be repaired by special-casing the elaboration
of a $\slet$-definition without an explicit type annotation. However, our
experience from playing with an implementation of this solution, is that we are
missing too much information by postponing, and this cascades in an unfortunate
way: postponing yields more unsolved metas, which cause more postponing.

\section{Telescopes and Strictly Curried Functions}

As part of the proposed solution, we extend the core theory with telescopes and
strictly curried functions. Figure \ref{fig:telescopes} lists the typing rules and
definitional equalities.

\subsection{Telescopes}
Telescopes can be viewed as a generic implementation of record types. We have
$\Tel$ as the type of telescopes. Elements of $\Tel$ are right-nested telescopes
of types, with $\epsilon$ denoting the empty telescope, and $\blank\TCons\blank$
telescope extension. For example, we can define the signature of natural number
algebras as follows:
\[
  \slet\,NatAlgSig : \Tel = (N : \U) \TCons (zero : N) \TCons (suc : N \to N) \TCons \epsilon\,\sin\,...
\]
We interpret an $A : \Tel$ as a record type as $\Rec\,A$, which behaves as the
evident iterated $\Sigma$-type corresponding to the telescope. Hence,
$\Rec\,\epsilon$ is isomorphic to the unit type, with inhabitant ${[}{]}$, and
$\Rec\,((x : A) \TCons B)$ behaves as a $\Sigma$-type, with pairing constructor
$\blank::\blank$ and projections $\pi_1$ and $\pi_2$. We also have the $\beta$
and $\eta$ rules for record constructors and projections in Figure
\ref{fig:telescopes}. We present definitional equalities in a compact form, but
note that they still stand for $\boxed{\Theta|\Gamma\vdash t \equiv u : A}$
judgments. Hence, the sides of the equations must have the same types, and in
particular the left side of the \LabTirName{${[}{]}$-$\eta$} rule has type
$\Rec\,\epsilon$.

Telescopes and records are derivable from natural numbers, the unit type and
$\Sigma$-types. We use Agda-like pattern matching notation in the following.
First, we define length-indexed telescopes.
\begin{alignat*}{3}
  & \rlap{$\Tel' : \Nat \to \U$}\\
  & \Tel'\,\zero      && :\equiv \top\\
  & \Tel'\,(\suc\,n)  && :\equiv \Sigma(A : \U).\,(A \to \Tel'\,n)
\end{alignat*}
Then, we have $\Tel :\equiv \Sigma(n : \Nat).\,(\Tel'\,n)$, and define records:
\begin{alignat*}{3}
  & \rlap{$\Rec : \Tel \to \U$}\\
  & \Rec\,(\zero,\,\_)        &&:\equiv \top\\
  & \Rec\,(\suc\,n,\,(A,\,B)) &&:\equiv \Sigma(a : A).\,(\Rec\,(n,\,B\,a))
\end{alignat*}
From the above, $\epsilon$, $\blank\TCons\blank$, $\blank::\blank$ and ${[}{]}$
are evident, and all expected equalities hold definitionally. Derivability is
good news because we inherit nice properties of the type theory which only
contains the base type formers. For instance, if we have a consistent universe
setup\footnote{Recall that we currently assume type-in-type, which causes consistency and normalization to fail.}, we inherit consistency, canonicity and decidability of conversion. We
currently use native telescopes instead of $\Nat$, $\top$ and $\Sigma$ because
in unification and elaboration it is convenient that we are able to restrict
some types to records types.

\begin{figure}[h]
\begin{mathpar}
  \inferrule*[lab=tel]
             {\\}
             {\Theta|\Gamma \vdash \Tel : \U}

  \inferrule*[lab=empty-tel]
             {\\}
             {\Theta|\Gamma \vdash \epsilon : \Tel}

  \inferrule*[lab=nonempty-tel]
             {\Theta|\Gamma \vdash A : \U \\ \Theta|\Gamma,\,x : A \vdash B : \Tel}
             {\Theta|\Gamma \vdash (x : A) \TCons B : \Tel}

  \inferrule*[lab=record-type]
             {\Theta|\Gamma \vdash A : \Tel}
             {\Theta|\Gamma \vdash \Rec\,A : \U}

  \inferrule*[lab=empty-record]
             {\\}
             {\Theta|\Gamma \vdash [] : \Rec\,\epsilon}

  \inferrule*[lab=nonempty-record]
             {\Theta|\Gamma \vdash t : A \\ \Theta|\Gamma \vdash u : \Rec\,(B[x\mapsto t])}
             {\Theta|\Gamma \vdash t :: u : \Rec\,((x : A) \TCons B)}

  \inferrule*[lab=record-projection]
             {\Theta|\Gamma \vdash t : \Rec\,((x : A) \TCons B)}
             {\Theta|\Gamma \vdash \pi_1\,t : A \\
               \Theta|\Gamma \vdash \pi_2\,t : \Rec\,(B[x\mapsto \pi_1\,t])}

  \inferrule*[lab=curried-fun]
             {\Theta|\Gamma \vdash A : \Tel \\ \Theta|\Gamma,\,x : \Rec\,A \vdash B : \U}
             {\Theta|\Gamma \vdash \{x : \ol{A}\} \to B : \U}

  \inferrule*[lab=curried-lam]
             {\Theta|\Gamma,\,x : \Rec\,A \vdash t : B}
             {\Theta|\Gamma \vdash \lambda\,\{x : \ol{A}\}.\,t : \{x : \ol{A}\} \to B}

  \inferrule*[lab=curried-app]
             {\Theta|\Gamma \vdash t : \{x : \ol{A}\} \to B \\ \Theta|\Gamma \vdash u : \Rec\,A}
             {\Theta|\Gamma \vdash t\,\{u : \ol{A}\} : B[x \mapsto u]}
\end{mathpar}

\begin{alignat*}{3}
  & \text{\LabTirName{$\pi_1$-$\beta$}}\hspace{2em}  && \pi_1\,(t :: u) && \equiv t\\
  & \text{\LabTirName{$\pi_2$-$\beta$}}  && \pi_2\,(t :: u) && \equiv u\\
  & \text{\LabTirName{$::$-$\eta$}}  && (\pi_1\,t :: \pi_2\,t) && \equiv t\\
  & \text{\LabTirName{${[}{]}$-$\eta$}}  && t && \equiv []\\
  & \text{\LabTirName{fun-$\epsilon$}}\hspace{3em} && \{x : \ol{\epsilon}\}\to B && \equiv B[x \mapsto []]\\
  & \text{\LabTirName{fun-$\TCons$}}         && \{x : \ol{(y : A) \TCons B}\}\to C \quad && \equiv
                                               \{y : A\}\to(\{b : \ol{B}\}\to C[x \mapsto (y :: b)])\\
  & \text{\LabTirName{lam-$\epsilon$}}       && \lambda\,\{x : \ol{\epsilon}\}.\,t && \equiv t[x \mapsto []]\\
  & \text{\LabTirName{lam-$\TCons$}}         && \lambda\,\{x : \ol{(y : A)\TCons B}\}.\,t && \equiv
                                         \lambda\{y\}.\,\lambda\,\{b : \ol{B}\}.\,t[x \mapsto (y :: b)]\\
  & \text{\LabTirName{app-$\epsilon$}}       && t\,\{u : \ol{\epsilon}\} && \equiv t\\
  & \text{\LabTirName{app-$\TCons$}}         && t\,\{u : \ol{(x : A) \TCons B}\} &&\equiv
                                         t\,\{\pi_1\,u\}\,\{\pi_2\,u : \ol{B[x \mapsto \pi_1\,u]}\}\\
  & \text{\LabTirName{curried-$\beta$}} && \lambda\,(\{x : \ol{A}\}.\,t)\,\{u:\ol{A}\} &&\equiv
                                                             t[x \mapsto u]\\
  & \text{\LabTirName{curried-$\eta$}}  && \lambda\,\{x : \ol{A}\}.\,t\,\{x : \ol{A}\} && \equiv t
\end{alignat*}

\caption{Rules for telescopes and strictly curried functions.}
\label{fig:telescopes}
\end{figure}


\subsection{Strictly Curried Functions}

These are function types whose domains are telescopes, and they are immediately
computed to iterated implicit function types when the domain telescope is
canonical.  See \LabTirName{fun-$\epsilon$} and \LabTirName{fun-$\TCons$}: a
curried function with empty domain computes to simply the codomain, while a
function with a non-empty domain computes to an implicit function type. We
explicitly notate telescopes in both $\lambda$-abstractions and applications for
strictly curried functions, since they are relevant in the computation rules.

Curried function types tend to be computed away, but they can persist if the
domain telescope is neutral, and in particular when it is meta-headed. For
example, assuming a meta $\alpha : \Tel$, the type $\{x : \ol{\alpha}\}\to B$
cannot be computed further. During elaboration, we will use strictly curried
function types to represent unknown insertions, but these types are eventually
computed away if a surface expression can be successfully elaborated. Since the
surface language remains unchanged, telescopes and curried functions are merely
an internal implementation detail from the perspective of programmers.

Curried functions are \emph{mostly} derivable from $\Nat$, $\top$ and $\Sigma$.
The type former is defined as follows:
\begin{alignat*}{3}
& \rlap{$\Pi^C : (A : \Tel) \to (\Rec\,A \to \U) \to \U$}\\
& \Pi^C (\zero,\,\_)\,B && :\equiv B\,\tt\\
& \Pi^C (\suc\,n,\,(A,\,B))\,C && :\equiv \{a : A\}\to \Pi^C\,(n,\,B\,a) (\lambda\,b.\,C\,(a,\,b))
\end{alignat*}
With this, we can also define $\mathsf{app} : \Pi^C\,A\,B \to (a : \Rec\,A) \to
B\, a$ and $\mathsf{lam} : ((a : \Rec\,A) \to B\, a) \to \Pi^C\,A\,B$, and all
equations in Figure \ref{fig:telescopes} hold definitionally, except
\LabTirName{curried-$\beta$} and \LabTirName{curried-$\eta$}. These do not hold
strictly, because $\Pi^C$, $\mathsf{app}$ and $\mathsf{lam}$ are all defined by
recursion on the $A$ telescope, but the $\beta\eta$ rules are specified
generically for arbitrary (possibly neutral)
telescopes. \LabTirName{curried-$\beta$} is still provable as a propositional
equality, and assuming function extensionality \LabTirName{curried-$\eta$} is
provable as well. For details, see our Agda formalization of these definitions,
which is included alongside the prototype implementation.

Hence, we can derive a somewhat weaker version of curried functions, with
propositional $\beta$ and $\eta$. From this, we still get consistency and
canonicity very cheaply. This is because models proving consistency or
canonicity for the base theory with $\Nat$, $\top$ and $\Sigma$ usually support
equality reflection. For consistency, standard set-theoretical models and models
in extensional type theory have this property, for canonicity, glued models
e.g.\ as in \cite{kaposi2019gluing, sterling2019algebraic} also have this property.

In contrast, showing normalization and decidability of conversion would require
some extra work. We leave this to future work, but we expect that it is not
difficult to extend previous proofs to cover strict $\beta$ and $\eta$ for
curried functions.

\section{Extending Elaboration}
\label{sec:extending_elaboration}

We shall utilize the extended core theory to implement smarter
elaboration. Recall from Section \ref{sec:issues} that the old elaborator makes
two kinds of unforced insertion choices:
\begin{enumerate}
\item $\echeck{t}{\Theta}{\Gamma}{A}$ does not insert an implicit $\lambda$ when $A$ is meta-headed.
\item $\einsert\,(\Theta,\,\theta,\,t,\,A)$ does not insert an implicit application when $A$ is meta-headed.
\end{enumerate}
In the following, we shall only enhance $\lambda$-insertions. This allows a
simple implementation which only requires minimal changes to unification, and
which is already remarkably powerful. It seems that enhancing implicit
application insertions requires extending unification; we discuss this in
Section \ref{sec:appinsert}. First, we modify closing types and
contextualization to take advantage of telescopes.

\begin{definition}[Closing types]
We use curried function types to close over record types in the scope. If a
bound variable does not have a record type, then we do as before\footnote{This
  implies that we close over meta-headed types using plain functions. In theory,
  this causes a higher-order version of the basic implicit insertion problem: we
  are uncertain about whether we should be uncertain about implicit
  insertions. So far, this higher-order insertion problem seems to be
  irrelevant in practice, in the prototype implementation.}. We prepend the
following clause to Definition
\ref{def:closingtype}:
\[
  ((\Gamma,\, x : \Rec\,A) \To B) :\equiv (\Gamma \To (\{x : \ol{A}\} \to B))
\]
\end{definition}
\begin{definition}[Contextualization]
We extend spine notation to applications of curried functions. For example, we
may have a spine $\ol{t} \equiv (\{x : \ol{A}\}\,\{y : \ol{B}\})$. We
accordingly revise Definition \ref{def:contextualization} for
$\overline{\mathsf{vars}_{\Gamma}}$ so that we use curried function application
for each record type in $\Gamma$.
\end{definition}

\subsection{Handling Superfluous Implicit Functions}\label{sec:superfluous}

Before we can move on to unification and elaboration, we have to address
a curious issue. Assuming $Bool : \U$, $true : Bool$ and $false : Bool$,
consider the following surface expression:
\[
  \slet\,x : \_ = true\,\sin\,x
\]
What should this expression elaborate to? We would expect the result to be simply
\[
  \slet\,x : Bool = true\,\sin\,x
\]
However, there are infinitely many core terms which are conservative over the
surface expression in the sense of Theorem \ref{thm:conservativity}. That is, we
can wrap definitions with any number of implicit $\lambda$-s, and add implicit
applications accordingly to usage sites of the defined name. For example, we could have
\[
  \slet\,x : \{y : Bool\} \to Bool = \lambda\,\{y\}.\,true\,\sin\,x\,\{true\}
\]
This is clearly undesirable. With the type $\{y : Bool\} \to Bool$, the implicit
argument $y$ is never inferable, because the codomain type does not depend on
the domain, and the argument is never constrained. Hence, with the above
definition, we always have to write $x\,\{true\}$ or $x\,\{false\}$ when we want
to use $x$. In order to avoid such nonsense, we adopt the following principle:
\emph{elaboration should never invent non-dependent implicit function types.}

This was a non-issue in the old elaborator, because it was not able to invent
implicit function types; it was only utilizing the type annotations present in
the surface input. In the case of $\slet\,x : \_ = true$, the old elaborator
checks $true$ with a fresh meta, and just assumes that the meta does not stand
for an implicit function type.

\subsubsection{Constancy constraints}
We use these constraints to get rid of curried function types as soon as we
learn that they are non-dependent. They are constraints in the usual sense in
unification algorithms (e.g.\ as in \cite{abel2011higher} or
\cite{vytiniotis2011outsidein}). We formalize them in a compact way, by adding a
new kind of context extension for metacontexts. The rules are given in Figure
\ref{fig:constancy}.

\begin{figure}
\begin{mathpar}
  \inferrule*[lab=metacon/constancy]
             {\Theta|\Gamma,\,x : \Rec\,A \vdash B : \U}
             {\Theta,\,\mathsf{constancy}_{\Gamma,\,x : \Rec\,A}\,B\vdash}

  \inferrule*[lab=constancy-$\equiv$]
             {x \notin \FreeVars(B)}
             {\Theta_0,\,\mathsf{constancy}_{\Gamma_0,\,x : \Rec\,A}\,B,\,\Theta_1|\Gamma_1 \vdash A \equiv \epsilon : \Tel}

  \inferrule*[lab=metasub/constancy]
             {\theta : \Theta_0 \To \Theta_1 \\\\ x \notin \FreeVars(B[\theta])\,\,\,\text{implies}\,\,\,\Theta_0|\Gamma[\theta]\vdash A[\theta] \equiv \epsilon : \Tel}
             {(\theta,\,\mathsf{solve}_{\Gamma,\,x : \Rec\,A}\,B) : \Theta_0 \To (\Theta_1,\,\constancy_{\Gamma,\,x : \Rec\,A}\,B)}

  \inferrule*[lab=metasub/weaken-constancy]
             {\\}
             {\p : (\Theta,\,\constancy_{\Gamma,\,x : \Rec\,A}\,B) \To \Theta}
\end{mathpar}
\caption{Rules for constancy constraints}
\label{fig:constancy}
\end{figure}

In the rule \LabTirName{metacon/constancy} we specify extension of a metacontext
with a constraint. The \LabTirName{constancy-$\equiv$} rule expresses that,
assuming we have a constancy constraint for $A$ and $B$ in context, if $B$ does
not depend on the $x : \Rec\,A$ domain variable, then $A$ is equal to the the
empty telescope $\epsilon$.

The \LabTirName{metasub/constancy} rule defines metasubstitutions whose
codomains are extended with constraints. Intuitively, while the
\LabTirName{metasub/extended} rule from Section \ref{sec:metasubstitutions} can
be used to solve a metavariable (by mapping it to a term),
\LabTirName{metasub/constancy} solves a constraint. We can only extend $\theta :
\Theta_0 \To \Theta_1$ to map into an additional constraint if $\theta$ forces
the constraint to hold. In \LabTirName{metasub/weaken-constancy}, we overload
$\p$ for the weakening substitution which drops a constraint.

\begin{definition}[Creating a new constraint]
We do this similarly to Definition \ref{def:freshmeta}, by simply
returning a weakening substitution.
\[
\newConstancy{\Theta}{\Gamma}{x}{A}{B} : \equiv
             ((\Theta,\,\constancy_{\Gamma,\,x : \Rec\,A}\,B),\,\p)
\]
\end{definition}

\subsubsection{Algorithmic implementation of constraint solving}
\label{sec:algorithmic}
The above specification for constancy constraints is compact but not
particularly algorithmic: we just magically get new definitional equalities
whenever we have constraints in contexts. In our prototype implementation, we
implement eager removal of solvable constraints.

After solving a meta $\alpha$ during unification, which yields a unifying
$\theta$ substitution, we review all ($\constancy_{\Gamma,\,x : \Rec\,A}\,B$)
constraints in the context, such that $x$ occurs in $B$ inside a $\ol{t}$ spine
of some $\alpha\,\ol{t}$ term. In other words, we review constraints where the
new meta solution might make a difference.
\begin{enumerate}
  \item If we have $x \in \FreeVars(B[\theta])$, where $x$ occurs rigidly in
    $B[\theta]$, i.e.\ the occurrence is not in a spine of a meta, then no
    metasubstitution can possibly remove this occurrence. In this case the
    constraint holds vacuously, so we can use the \LabTirName{metasub/constancy}
    rule to return a $\theta'$ substitution which also solves the constraint.
  \item If we have $x \notin \FreeVars(B[\theta])$, we recursively unify
    $A[\theta]$ with $\epsilon$. If that succeeds, we get a $\theta'$ which
    unifies $A[\theta]$ and $\epsilon$ and thus forces the constraint to hold, so we
    can again use \LabTirName{metasub/constancy} to solve the constraint.
  \item
    In any other case we simply return $\theta$ and keep the constraint around.
\end{enumerate}
Also, when we create a new constancy constraint, we immediately review it
as described above.

\emph{Remark.} In the case with $x \in \FreeVars(B[\theta])$, it would be also
sound to solve the constraint when the occurrence is not rigid. However, this
way we could lose potential non-$\epsilon$ solutions of $A[\theta]$.

\subsection{Unification For Strictly Curried Functions}
Although we omit most details of unification, we shall discuss it for curried
functions, as it is essential in the extended elaboration algorithm.  The most
interesting case is when we unify a curried function type with an implicit
function type. In this case, we learn that the domain of the curried function is
non-empty, so we refine the $A$ domain to an extended $(x_0 : A_0) \TCons A_1$
telescope. Since we invent a fresh $A_1$ domain for a curried function type, we
need to add a constancy constraint for it as well.
\begin{alignat*}{3}
& \unify_{\Theta_0|\Gamma}(\{x : \ol{A}\}\to B)\,(\{x_0 : A_0\}\to B') :\equiv \edo\\
& \quad \slet\,(\Theta_1,\,\theta_1,\,A_1) = \freshMeta{\Theta_0}{\Gamma,\,x_0 : A_0}{\Tel}\\
& \quad (\Theta_2,\,\theta_2) \leftarrow
  \unify_{\Theta_1|\Gamma[\theta_1]}\,(A[\theta_1])\,((x : A_0[\theta_1])\TCons A_1)\\
& \quad \slet\,(\Theta_3,\,\theta_3) =
    \newConstancy{\Theta_2}{\Gamma[\theta_{12}],\,x_0 : A_0[\theta_{12}]}{x_1}{(A_1[\theta_2])}{(B[\theta_{12}][x\mapsto (x_0 :: x_1)])}\\
& \quad\unify_{\Theta_3|\Gamma[\theta_{123}],\,x_0 : A_0[\theta_{123}]}\,
         (\{x_1 : A_1[\theta_{23}]\}\to B[\theta_{123}][x\mapsto (x_0 :: x_1)])\,B'
\end{alignat*}
We have the symmetric $\unify_{\Theta_0|\Gamma}(\{x_0 : A_0\}\to B')\,(\{x :
\ol{A}\}\to B)$ case the same way as above.

Now, let us assume that $B'$ is not an implicit function type, curried function
type or meta-headed. Then, we have the following case, where we solve a
telescope domain to be empty.
\begin{alignat*}{3}
& \unify_{\Theta_0|\Gamma}(\{x : \ol{A}\}\to B)\,B' :\equiv \edo\\
& \quad (\Theta_1,\,\theta_1) \leftarrow \unify_{\Theta_0|\Gamma}\,A\,\epsilon\\
& \quad \unify_{\Theta_1|\Gamma[\theta_1]}\,(B[\theta_1][x\mapsto[]])\,(B'[\theta_1])
\end{alignat*}
Again, we also have the symmetric case. For $\lambda\,\{x : \ol{A}\}.\,t$ and
$t\,\{u : \ol{A}\}$, unification is structural, and other cases remain the same
as in the the basic elaborator of Section \ref{sec:bidirectional_elaboration}.

\subsection{Elaboration}

In the definition of checking, we insert a new clause after
$\echeck{t}{\Theta}{\Gamma}{(\{x : A\}\to B)}$:
\begin{alignat*}{3}
& \echeck{t}{\Theta_0}{\Gamma}{(\alpha\,\ol{u})} :\equiv \edo\\
& \quad \slet\,(\Theta_1,\,\theta_1,\,A) = \freshMeta{\Theta_0}{\Gamma}{\Tel}\\
& \quad (\Theta_2,\,\theta_2,\,t',\,B) \leftarrow \einfer{t}{\true}{\Theta_1}{\Gamma[\theta_1],\,x:\Rec\,A}\\
& \quad \slet\,(\Theta_3,\,\theta_3) = \newConstancy{\Theta_2}{\Gamma[\theta_{12}]}{x}{(A[\theta_2])}{B}\\
& \quad (\Theta_4,\,\theta_4) \leftarrow
            \unify\, ((\alpha\,\ol{u})[\theta_{123}])\,(\{x : \ol{A[\theta_{23}]}\} \to B[\theta_3])\\
& \quad \ereturn\,(\Theta_4,\,\theta_{1234},\,(\lambda\,\{x : \ol{A[\theta_{234}]}\}.\,t'[\theta_{34}]))
\end{alignat*}
Hence, when checking a term with a meta-headed type, we create a fresh meta with
$\Tel$ type, infer a type for the term, and wrap the result in a strictly
curried $\lambda$. We again need to create new constancy constraint. This way,
if $B$ does not depend on $x : \Rec\,A$, the strictly curried $\lambda$ in the
output immediately computes away, since $A$ is solved to $\epsilon$.

This concludes the definition of the extended elaborator. The new algorithm is
sound with respect to the extended core syntax, and it also has the
conservativity property from Theorem \ref{thm:conservativity} if we additionally
allow elaboration to insert $\lambda$-s for strictly curried functions. We present
some examples of the algorithm in action.


\begin{example}
We return to the $polyList$ example from Section \ref{sec:issues}. We trace
elaboration using the extended algorithm.
\begin{alignat*}{3}
  & \scriptstyle{0 }\qquad\qquad && \echeckt{cons\,(\lambda\,x.\,x)\,nil}{(List\,(\{A : \U\}\to A \to A))}
      \hspace{14em}\\
  & \scriptstyle{1 }  && \quad \einfert{cons\,(\lambda\,x.\,x)\,nil} \\
  & \scriptstyle{2 }  && \qquad \einfert{cons\,(\lambda\,x.\,x)} \\
  & \scriptstyle{3 }  && \qquad\quad \einfert{cons}\\
  & \scriptstyle{4 }  && \qquad\quad = cons\,\{\alpha_0\}\,:\,\alpha_0 \to List\,\alpha_0 \to List\,\alpha_0\\
  & \scriptstyle{5 }  && \qquad\quad \echeckt{\lambda\,x.\,x}{\alpha_0}\\
  & \scriptstyle{6 }  && \qquad\quad = \lambda\,\{y : \ol{\alpha_1}\}.\,\lambda\,x.\,x\\
  & \scriptstyle{7 }  && \qquad = cons\,
                       \{\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}\}\,
                       (\lambda\,\{y : \ol{\alpha_1}\}.\,\lambda\,x.\,x)\\
  & \scriptstyle{8 }  && \qquad \echeckt{nil}{(List\,(\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}))}\\
  & \scriptstyle{9 }  && \qquad = nil\,\{\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}\}\\
                       & \scriptstyle{10} && \quad = cons\,\{\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}\}\,(\lambda\,\{y : \ol{\alpha_1}\}.\,\lambda\,x.\,x)\,\\
                       & && \hspace{4.4em} (nil\,\{\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}\})\\
  & && \hspace{3em} : List\,(\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\})\\
  & \scriptstyle{11} && \quad \unify\,(List\,(\{A : \U\}\to A \to A))\,(List\,(\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\}))\\
  & \scriptstyle{12} && \qquad \unify\,(\{A : \U\}\to A \to A)\,(\{y : \ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y : \ol{\alpha_1}\})\\
  & \scriptstyle{13} && \qquad\quad \unify\,\alpha_1\,((A : \U) \TCons (\alpha_3\,A))\\
  & \scriptstyle{14} && \qquad \quad \unify\,(A \to A)\,(\{z : \ol{\alpha_3\,A}\} \to \alpha_2\,\{A\}\,\{z : \ol{\alpha_3\,A}\}\to \alpha_2\,\{A\}\,\{z : \ol{\alpha_3\,A}\})\\
  & \scriptstyle{15} && \qquad \qquad \unify\,(\alpha_3\,A)\,\epsilon\\
  & \scriptstyle{16} && \qquad \qquad \unify\,(A \to A)\,(\alpha_2\,\{A\} \to \alpha_2\,\{A\})\\
  & \scriptstyle{17} && \qquad \qquad \quad \unify\,A\,(\alpha_2\,\{A\})\\
  & \scriptstyle{18} && \qquad \qquad \quad \unify\,A\, A\\
  & \scriptstyle{19} && = cons\,\{\{A : \U\}\to A \to A\}\,(\lambda\,\{A\}\,x.\,x)\,(nil\,\{\{A : \U\}\to A \to A\})
\end{alignat*}
We diverge from the previous attempt at line $\scriptstyle{5}$. Here, we check
$\lambda\,x.\,x$ with the meta $\alpha_0$, so we create a fresh $\alpha_1 :
\Tel$ meta and wrap the result as $\lambda\,\{y :
\ol{\alpha_1}\}.\,\lambda\,x.\,x$. This result has the inferred type $\{y :
\ol{\alpha_1}\} \to \alpha_2\,\{y : \ol{\alpha_1}\}\to \alpha_2\,\{y :
\ol{\alpha_1}\}$, and we promptly unify $\alpha_0$ (which stands for the list
element type) with it. On line $\scriptstyle{10}$, we return the elaborated
$cons$ expression, where the list element type is made explicit. It only remains
to unify the inferred and expected types. On line $\scriptstyle{12}$ we have the
case where a curried function type is matched with an implicit function type, so
on line $\scriptstyle{13}$ we refine $\alpha_1$ to a non-empty telescope, and
proceed unifying the domains. Note that on line $\scriptstyle{14}$ we have
$\alpha_2\,\{A\}\,\{z : \ol{\alpha_3\,A}\}$, which results from applying the
substitution $y \mapsto (A :: z)$, and computing $\alpha_2\,\{(A :: z) : \ol{(A :
  \U) \TCons (\alpha_3\,A)}\}$ further using the \LabTirName{app-$\TCons$} rule
from Figure \ref{fig:telescopes}.

On line $\scriptstyle{14}$ we have the case when a curried function type is
matched with a type which is not a curried function, implicit function or a
meta-headed type. We accordingly unify domain telescope $\alpha_3\,A$ with
$\epsilon$, which causes $\alpha_3$ to be solved as $\lambda\,A.\,\epsilon$
according to standard pattern unification. Now, $\alpha_2\,\{A\}\,\{z :
\ol{\alpha_3\,A}\}$ computes to $\alpha_2\,\{A\}\,\{z : \epsilon\}$, which
further computes to $\alpha_2\,\{A\}$ by \LabTirName{app-$\epsilon$}. From here,
unification finishes with success. On the last line, the expected output is
returned: since the $\alpha_1$ telescope meta is now solved, curried function
types and abstractions are computed away.
\end{example}

%% \begin{alignat*}{3}
%% & \scriptstyle{0}\qquad\qquad && \einfert{\slet\,x : \_ = true\,\sin\,x}{\true}
%%       \hspace{21em}\\
%% & \scriptstyle{1} && \quad \echeckt{\_}{\U}\\
%% & \scriptstyle{2} && \quad = \alpha_0\\
%% & \scriptstyle{3} && \quad \echeckt{true}{\alpha_0}\\
%% & \scriptstyle{4} && \qquad \mathsf{freshMeta}\,\Tel\\
%% & \scriptstyle{5} && \qquad = \alpha_1\\
%% & \scriptstyle{6} && \qquad \einfert{true}{\true}\\
%% & \scriptstyle{7} && \qquad = true : Bool\\
%% & \scriptstyle{8} && \qquad \mathsf{newConstancy}_{\emptycon,\,x : \Rec\,\alpha_1}\,Bool\\
%% & \scriptstyle{9} && \qquad\quad \unify\,\alpha_1\,\epsilon\\
%% & \scriptstyle{10} && \qquad \unify\,\alpha_0\,Bool\\
%% & \scriptstyle{11} && \quad = true\\
%% & \scriptstyle{12} && \quad \einfert{x}{\true}\\
%% & \scriptstyle{13} && \quad = true : Bool\\
%% & \scriptstyle{14} && = (\slet\,x : Bool = true\,\sin\,x) : Bool
%% \end{alignat*}

\begin{example}
We illustrate now the action of constancy constraints, using the example from
Section \ref{sec:superfluous}. Again, assume $Bool : \U$ and $true : Bool$.
\begin{alignat*}{3}
& \scriptstyle{0}\qquad\qquad && \einfert{\slet\,x : \_ = true\,\sin\,x}
      \hspace{21em}\\
& \scriptstyle{1} && \quad \echeckt{\_}{\U} = \alpha_0\\
& \scriptstyle{3} && \quad \echeckt{true}{\alpha_0}\\
& \scriptstyle{4} && \qquad \mathsf{freshMeta}\,\Tel = \alpha_1\\
& \scriptstyle{6} && \qquad \einfert{true}\,= true : Bool\\
& \scriptstyle{8} && \qquad \mathsf{newConstancy}_{\emptycon,\,x : \Rec\,\alpha_1}\,Bool\\
& \scriptstyle{9} && \qquad\quad \unify\,\alpha_1\,\epsilon\\
& \scriptstyle{10} && \qquad \unify\,\alpha_0\,Bool\\
& \scriptstyle{11} && \quad = true\\
& \scriptstyle{12} && \quad \einfert{x}{\true} = true : Bool\\
& \scriptstyle{14} && = (\slet\,x : Bool = true\,\sin\,x) : Bool
\end{alignat*}
First we create a fresh meta $\alpha_0$ for the hole on line $\scriptstyle{2}$,
then we check $\true$ with it. Since we are checking with a meta-headed type, we
create a fresh telescope meta $\alpha_1$, then infer $Bool$ for $true$. On line
$\scriptstyle{8}$ we create a constancy constraint and immediately try to solve
it, as described in Section \ref{sec:algorithmic}. Since $Bool$ does not depend
on the domain, we solve the constraint and thereby solve $\alpha_1$ as
$\epsilon$. Hence, we simply return $true$ on line $\scriptstyle{11}$, since
$\lambda\,\{x : \ol{\epsilon}\}.\,true$ computes to that, and elaboration
succeeds with the expected output.
\end{example}

\section{Implementation}
\label{sec:impl_and_eval}

We provide an implementation of the elaborator from Section
\ref{sec:extending_elaboration}. It is a standalone Haskell program which reads
a surface expression from standard input, and outputs the result of elaboration,
or optionally the type or the normal form of the result. It is implemented in
1056 lines of Haskell. Of this, 669 lines constitute the core syntax,
evaluation, unification and elaboration. Of these 669 lines, 454 lines implement
the basic elaborator of Section \ref{sec:bidirectional_elaboration} and 215
lines implement the extended elaborator.

Elaboration is implemented in the style of Coquand's algorithm
\cite{coquand1996algorithm}, where elaboration is interleaved with
normalization-by-evaluation. Hence, we do not perform any substitution
operations on core syntax, instead we evaluate core terms into the semantic
domain, and perform unification, strengthening and occurrence checking on
semantic values. There is also a quoting (or readback) operation which yields
normalized core terms from values, and which is used most prominently when we
have to generate solutions for metavariables.

Metacontexts are a key point in the implementation. We avoid the tedious (and
inefficient) threading of composed metasubstitutions, instead we have a mutable
reference which stores the current metasubstitution. We have a ``forcing''
operation which computes a semantic value to a head normal form with respect to
the current metasubstitution. Hence, instead of constantly updating every term
and type by performing metasubstitution, we only force them whenever we need to
pattern match on the shape of types, for example when we are inserting implicit
arguments based on types. In this paper we still stick with the ``threaded
metasubstitution'' presentation because it is more conventional, and also more
straightforward to formalize.

We use normalization-by-evaluation in Abel's style (see
e.g.\ \cite[Chapter~3]{abel2013normalization}), where semantic values use de
Bruijn levels, and the core syntax uses de Bruijn indices. This is practically
very favorable, because the evaluator never has to perform weakening on values.
The implementation of curried functions presents a bit of a complication,
because the computation rules are type-directed, so we have to annotate
applications and abstractions with telescopes (just as in our notation for the
core syntax). We implement constraints in a fairly optimized way: we keep track
of relevant ``blocking'' metas for each constraint and upon solving a meta we
only review constraints which were blocked on the meta.

\section{Related Works and Evaluation}
\label{sec:impl_and_eval}

In this section we examine how the elaborator fares in practice, its
limitations, and how it compares to related works. We refer to our elaborator as
FCIF when comparing it to others.

\subsection{Related Works}
MLF \cite{le2014mlf} extends System F with polymorphic subtyping
bounds, and supports strong inference for first class polymorphism. HML
\cite{leijen2009flexible}) is a simplified variant of MLF. These systems rely
critically on subtyping and thus diverge markedly from (fragments of) Martin-Löf
type theory. HMF \cite{leijen2008hmf} is a relatively simple system with
first-class polymorphism; however, it is also weaker than others in terms of
inferable annotations, and its more powerful variant (extended with n-ary
application handling) is more complex.

There have been several attempts in the context of GHC. Boxy types
\cite{vytiniotis2006boxy} and FPH \cite{vytiniotis2008fph} were two early
iterations which suffered from complex specification, complex implementation or
fragility. More recent works are guarded impredicativity (GI)
\cite{serrano2018guarded} and quick look impredicativity (QL)
\cite{serrano2020a}. They both work by examining n-ary applications to find
metavariable instantiations which arise from occurrences guarded by rigid type
constructors. GI's use of \emph{generalization constraints} anticipates our use
of telescopes, but overall GI is also burdened by a great deal of complexity. QL
streamlines GI by eschewing constraints in favor of an eager preprocessing pass
on neutral expressions which finds polymorphic instantiations. QL also takes
advantage of bidirectional type propagation.  Although QL seems to be
practically the most favorable so far, it is far from being elegant: it rechecks
expressions multiple times, and the restriction of preprocessing to (nested)
neutral applications is rather ad-hoc.


{\small

\begin{figure}
\begin{alignat*}{4}
  & IdTy   && :\equiv \{A : \U\}\to A \to A   \hspace{10em}         && inc    && : Int \to Int                                        \\
  & single && : \{A : \U\} \to A \to List\,A                       && auto   && : IdTy \to IdTy                                      \\
  & id     && : IdTy                                               && auto'  && : \{B : \U\}\to IdTy \to B \to B                     \\
  & ids    && : List\,IdTy                                         && choose && : \{A : \U\} \to A \to A \to A                       \\
  & nil    && : \{A : \U\}\to List\,A                              && app    && : \{A\,B : \U\} \to (A \to B) \to A \to B            \\
  & cons   && : \{A : \U\} \to A \to List\,A \to List\,A           && revapp && : \{A\,B : \U\} \to A \to (A \to B) \to B            \\
  & head   && : \{A : \U\} List\,A \to A                           && runST  && : \{A : \U\} \to (\{S : \U\} \to ST\,S\,A) \to A     \\
  & tail   && : \{A : \U\} List\,A \to List\,A                     && argST  && : \{S : \U\}\to ST\,S\,Int                           \\
  & map    && : \{A B : \U\} \to (A \to B) \to List\,A \to List\,B && poly   && : IdTy \to Pair\,Int\,Bool
\end{alignat*}
\caption{Types used in Figure \ref{fig:benchmark}.}
\label{fig:benchmark_types}
\end{figure}

\begin{figure}
\begin{tabular}{|lll|}
\hline
A & \multicolumn{2}{l|}{\textsc{polymorphic instantiation}}\\
\hline
A1 & $\lambda\,x\,y.\,y$ & \yesst\\
   & \text{we infer a type which may be solved to $\{A\,B : \U\}\to A \to B \to A$} &\\
   & \text{but not to $\{A : \U\}\to A \to \{B : \U\} \to B \to A$} & \\
A2 & $choose\,id$ & \yesst \\
A3 & $choose\,nil\,ids$ & \yes \\
A4 & $\lambda\,(x : \{A : \U\}\to A \to A).\,x\,x$ & \yesst\\
A5 & $id\,auto$ & \yes\\
A6 & $id\,auto'$ & \yesst\\
A7 & $choose\,id\,auto$ & \yes\\
A8 & $choose\,id\,auto'$ & \no\\
A9 & $\lambda\,(f : \{A : \U\}\to(A\to A)\to List\,A \to A).\,f\,(choose\,id)\,ids$ & \yesst\\
A10 & $poly\,id$ & \yes\\
A11 & $poly\,(\lambda\,x.\,x)$ & \yes\\
\hline
B & \multicolumn{2}{l|}{\textsc{inference of polymorphic arguments}}\\
\hline
B1 & $\lambda\,f.\,pair\,(f\,zero)\,(f\,true)$ & \no\\
B2 & $\lambda\,xs.\,poly\,(head\,xs)$ & \no\\
\hline
C & \multicolumn{2}{l|}{\textsc{functions on polymorphic lists}}\\
\hline
C1  & $length\,ids$ & \yes \\
C2  & $tail\,ids$ & \yes \\
C3  & $head\,ids$ & \yesst \\
C4  & $single\,id$ & \yesst \\
C5  & $cons\,id\,ids$ & \yes \\
C6  & $cons\,(\lambda\,x.\,x)\,ids$ & \yes \\
C7  & $append\,(single\,inc)\,(single\,id)$ & \yes \\
C8  & $\lambda\,(g:\{A:\U\}\to List\,A \to List\,A \to A).\,g\,(single\,id)\,ids$ & \yesst \\
C9  & $map\,poly\,(single\,id)$ & \yes \\
C10 & $map\,head\,(single\,ids)$ & \yes \\
\hline
D & \multicolumn{2}{l|}{\textsc{application functions}}\\
\hline
D1 & $app\,poly\,id$        & \yes \\
D2 & $revapp\,id\,poly$     & \yes \\
D3 & $runST\,argST$         & \yes \\
D4 & $app\,runST\,argST$    & \yes \\
D5 & $revapp\,argST\,runST$ & \yes \\
\hline
E & \multicolumn{2}{l|}{\textsc{$\eta$-expansion}}\\
\hline
& \multicolumn{2}{l|}{assuming $k : \{A : \U\}\to A \to List\,A \to A$,} \\
& \multicolumn{2}{l|}{$h : Int \to \{A:\U\}\to A \to A$}\\
& \multicolumn{2}{l|}{and $lst : List\,(\{A:\U\} \to Int \to A \to A)$} \\
E1 & $k\,h\,lst$ & \no \\
E2 & $k\,(\lambda\,x.\,h\,x)\,lst$ & \yesst \\
E3 & $\lambda\,(r : (\{A :\U\}\to A \to \{B : \U\}\to B \to B) \to Int).\,r\,(\lambda\,x\,y.\,y)$ & \yes \\
\hline
\multicolumn{3}{l}{\text{{\footnotesize
``Yes*'' means that our system can infer a type for the
expression, but the lack of}}}\\
\multicolumn{3}{l}{\text{{\footnotesize
let-generalization yields unsolved metas in the type.}}}
\end{tabular}

\caption{Elaboration benchmark from \cite{serrano2018guarded}.}
\label{fig:benchmark}
\end{figure}
}

\subsection{Evaluation}
We borrow a very useful collection of inference benchmarks from the GI
\cite{serrano2018guarded} and QL \cite{serrano2020a} papers: see Figure
\ref{fig:benchmark_types} and Figure \ref{fig:benchmark}. We also include a
source file in the implementation which reproduces these results.  In ibid.\ a
comparison is presented between multiple systems, but here we only include
FCIF. The relative performance of FCIF here is easy to remember: it handles
exactly the same cases as QL (which is slightly more than what GI covers). We
mark some cases as ``Yes*'', where FCIF successfully infers a type, but since
FCIF does no let-generalization, the inferred types are not fully constrained
without extra contextual information.

In the $A1$ case, FCIF inserts only a single curried $\lambda$ on the outside,
which is why the inferred type is not unifiable with $\{A : \U\}\to A \to \{B :
\U\} \to B \to A$. However, this could be easily remedied by adding an extra
curried $\lambda$ insertion in the definition of
$\einfer{\lambda\,x.\,t}{\Theta}{\Gamma}$.

In $A8$, the failure is intentional: the types of $id$ and $auto'$ are not
unifiable because of the mismatched order of implicit and explicit arguments. We
do not float out or reorder implicit arguments in any way, because we want to
support arbitrary mixing of implicit/explicit arguments in the surface language,
and such reordering would be problematic anyway in the presence of dependent
types. The same situation arises in $E1$, where we cannot unify $\{A : \U\}\to
Int \to A \to A$ with $Int \to \{A : \U\}\to A \to A$.

Some general comments on the comparison of FCIF to prior works. First, FCIF
supports dependent types and prior solutions do not. It is also unclear whether
prior solutions can scale to dependent types. MLF and HML rely on subtyping, and
solutions which work one neutral spine at a time (HMF, GI, QL) also face issues
with dependently typed spines. With such spines, we cannot simply process later
arguments when previous ones are not yet elaborated, as return types depend on
argument values, and skipping over arguments may clog up type computation in an
unacceptable way.

Secondly, FCIF supports global inference. For an example for global inference,
$\slet\,x : \_ = single\,id\,\sin\,$ $cons\,\{IdTy\}\,x\,nil$ works in FCIF, MLF
and HMF but does not work in GI, QL and HMF. MLF and HML work here by
immediately giving a principal type to $single\,id$ which involves
generalization with a subtyping bound. In contrast, FCIF makes no promises about
principal typing --- which is not feasible with dependent types --- and does no
generalization, but it can still infer a type for $id$ which can be later
constrained to $IdTy$.


\subsection{Inferring Polymorphic Types for Function Arguments}
\label{sec:appinsert}

It is apparent from the $B1$ and $B2$ cases on Figure \ref{fig:benchmark} that FCIF
cannot infer implicit function types for function arguments.  MLF is the only
system which can do this, in cases where the polymorphic argument is used only
once. Consider the following:
\[
  \slet\,f : IdTy \to Bool = \lambda\,\_.\,true\,\sin\,\lambda\,x.\,f\,x
  \]
FCIF will attempt to elaborate $f\,x$ to $f\,(\lambda\{A\}.\,x)$, but fails to
infer a type for $x$, because $A$ is not in the scope of $x$'s type. We sketch
an extension of FCIF with \emph{curried application insertion}, which could
possibly handle this example. We extend insertion with the following case. We
omit metacontexts and metasubstitutions for brevity.
\begin{alignat*}{3}
& \einsert\,(t,\,\alpha\,\ol{u}) :\equiv \edo \\
& \quad A \leftarrow \mathsf{freshMeta}_{\Gamma}\,\Tel\\
& \quad B \leftarrow \mathsf{freshMeta}_{\Gamma,\,x: \Rec\,A}\,\U\\
& \quad \unify\,(\alpha\,\ol{u})\,(\{x : \ol{A}\} \to B)\\
& \quad \mathsf{newConstancy}_{\Gamma, x : \Rec\,A}\,B\\
& \quad u \leftarrow \mathsf{freshMeta}_{\Gamma}\,(\Rec\,A)\\
& \quad \ereturn\,(t\,\{u : \ol{A}\},\,B[x \mapsto u])
\end{alignat*}
In short, we insert a new curried application whenever we $\einsert$ with
meta-headed type. Now, elaboration returns
$f\,(\lambda\,\{A\}.\,x\,\{\alpha_2\,x\,A : \ol{\alpha_0}\})$, and right before
we try to unify expected and inferred types, we have $x : \{y :
\ol{\alpha_0}\}\to \alpha_1\,\{y : \ol{\alpha_0}\}$, and we have to compute
$\unify\,(\alpha_1\,\{\alpha_2\,x\,A : \ol{\alpha_0}\})\,(A \to A)$. This
problem does not fall in any usual pattern fragment, and does not have an
obvious most general solution. However, it is reasonable to make the following
assumption: metas which return in unknown record types, like $\alpha_2$, are
only solved with \emph{order-preserving embeddings}, so for $\alpha_2$ the four
potential solutions are $(\lambda\,x\,A.\,[])$, $(\lambda\,x\,A.\,(A :: []))$,
$(\lambda\,x\,A.\,(x :: []))$ and $(\lambda\,x\,A.\,(x :: A :: []))$\footnote{Technically, we could also allow permutations such as $\lambda\,x\,A.\,(A :: x :: [])$, but these are superfluous since nothing in our system depends on the ordering of permutable function arguments.}. The reason
for only allowing such solutions is that these can possibly yield solvable
pattern unification problems, while solutions containing arbitrary terms or
non-linear variable occurrences cannot. Out of these, $(\lambda\,x\,A.\,(x :: A
:: []))$ and $(\lambda\,x\,A.\,(x :: []))$ are ruled out because they would
require a cyclic solution for $\alpha_0$, and $(\lambda\,x\,A.\,[])$ is ruled
out because it would yield the unsolvable $\unify\,\alpha_1\,(A \to A)$
problem. Thus, $(\lambda\,x\,A.\,(A :: []))$ is the unique order-preserving
embedding solution in this case, which does yield the expected elaboration
output. However, this idea is far from being fully fleshed out, and it
could be subject of future research.

\section{Conclusion and Future Work}

In type theory, it is a common endeavor to search for theories and features
which confer the greatest amount of expressive power for the least amount of
formal and conceptual complexity. In relation to elaboration and inference
algorithms, we would similarly like to use tools and concepts with high
power-to-weight ratio. Sometimes core theories need to be extended to allow more
powerful elaboration. Certainly, type inference would be cumbersome without the
notion of metavariables. Likewise, contextual metavariables are essential
for elaboration in the presence of type dependencies.

In this paper we propose the concept of strictly curried functions, which
supports elaboration of first-class implicit functions. It seems that trying to
solve this problem by fiddling with postponing and heuristics, without extending
the core theory, does not really cut it. In a dependently typed spine $t\,u\,v$, when
elaborating $v$ we already want to have an elaborated version of $u$ with
computational behavior. While contextual metavariables yield a nice modal type
theory which computes in the presence of \emph{unknown terms}, our curried
functions extend this to a system which also computes in the presence of
unknown \emph{implicit insertions}. Since implicit insertions are in a way also
just unknown terms, we can reuse most of the infrastructure of contextual/crisp
type theory, and only add modest extensions.

Often it is the case that constructions on dependent type theory are forced to
be more principled and structured than constructions on less powerful theories,
because the many interactions and intricacies leave less room for whims. In our
case, we believe that focusing on dependent type theory helped us hone in on the
essential parts of the problem, and it is likely that restrictions of our
solution would be also favorably simple and powerful in simpler settings such as
System F.

In future work, we would like to
\begin{itemize}
\item Investigate extending elaboration with curried application insertions, along the lines
      of Section \ref{sec:appinsert}.
\item Formalize elaboration and unification from an algebraic/categorical perspective, in particular
      give algebraic definitions for the core theories.
\item Investigate whether features of our elaborator could be implemented in a
      reasonably practical way in production systems such as Agda. Also, investigate
      simplifying our algorithm to non-dependent (e.g.\ System F) settings.
\item Investigate completeness and properties related to inferability.
\end{itemize}


\begin{acks}
  This work was supported by the European Union, co-financed by the
  European Social Fund (EFOP-3.6.3-VEKOP-16-2017-00002) and COST Action
  EUTypes CA15123.
\end{acks}

%% Bibliography
\bibliography{references}

\end{document}
